{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6f90f230-01e9-49ca-936d-79b61a1a6865",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total chars: 102463\n",
      "﻿The Project Gutenberg eBook of To Nuremberg and Back: A Girl's Holiday\n",
      "    \n",
      "This ebook is for the \n"
     ]
    }
   ],
   "source": [
    "# read in the test data that we are going to tokenize\n",
    "with open(\"gutten.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    raw_text = f.read()\n",
    "print(f\"Total chars: {len(raw_text)}\")\n",
    "print(raw_text[:99])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "21b6413a-eb79-4497-adb0-ddfd22841685",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', ',', '', ' ', 'world', '.', '', ' ', 'This', '--', '', ',', '', ' ', 'is', ' ', 'a', ' ', 'test', '?', '']\n",
      "['Hello', ',', 'world', '.', 'This', '--', ',', 'is', 'a', 'test', '?']\n"
     ]
    }
   ],
   "source": [
    "# basic regex parsing\n",
    "import re\n",
    "text = \"Hello, world. This--, is a test?\"\n",
    "result = re.split(r'([,.:;?_!\"()\\']|--|\\s)', text)\n",
    "print(result)\n",
    "result = [item for item in result if item.strip()]\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3ba9bc73-b08b-4dfb-abb6-e6669843b21a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20970\n",
      "['\\ufeffThe', 'Project', 'Gutenberg', 'eBook', 'of', 'To', 'Nuremberg', 'and', 'Back:', 'A', 'Girl', \"'\", 's', 'Holiday', 'This', 'ebook', 'is', 'for', 'the', 'use', 'of', 'anyone', 'anywhere', 'in', 'the', 'United', 'States', 'and', 'most', 'other']\n"
     ]
    }
   ],
   "source": [
    "preprocessed = re.split(r'([,.?_!\"()\\']|--|\\s)', raw_text)\n",
    "preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
    "print(len(preprocessed))\n",
    "print(preprocessed[:30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1b17b29e-a125-45c8-9229-d453a7576ba4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3370\n",
      "('!', 0)\n",
      "('\"', 1)\n",
      "('#45983]', 2)\n",
      "('$1', 3)\n",
      "('$5', 4)\n",
      "('&', 5)\n",
      "(\"'\", 6)\n",
      "('(', 7)\n",
      "(')', 8)\n",
      "('***', 9)\n",
      "(',', 10)\n",
      "('-', 11)\n",
      "('--', 12)\n",
      "('.', 13)\n",
      "('000', 14)\n",
      "('1', 15)\n",
      "('1006', 16)\n",
      "('101', 17)\n",
      "('1015', 18)\n",
      "('102', 19)\n",
      "('1050', 20)\n",
      "('108', 21)\n",
      "('109', 22)\n",
      "('11', 23)\n",
      "('1105', 24)\n",
      "('111', 25)\n",
      "('112', 26)\n",
      "('1127', 27)\n",
      "('113:', 28)\n",
      "('1156', 29)\n",
      "('1188', 30)\n",
      "('1274', 31)\n",
      "('1356', 32)\n",
      "('1361', 33)\n",
      "('1385', 34)\n",
      "('1450', 35)\n",
      "('1489', 36)\n",
      "('1490', 37)\n",
      "('15', 38)\n",
      "('1500', 39)\n",
      "('1508', 40)\n",
      "('1519', 41)\n",
      "('1555', 42)\n",
      "('1568', 43)\n",
      "('1601', 44)\n",
      "('1616', 45)\n",
      "('1619', 46)\n",
      "('1649', 47)\n",
      "('18', 48)\n",
      "('1806', 49)\n",
      "('1835', 50)\n",
      "('1840', 51)\n"
     ]
    }
   ],
   "source": [
    "# start converting tokens to integers\n",
    "all_words = sorted(list(set(preprocessed)))\n",
    "vocab_size = len(all_words)\n",
    "print(vocab_size)\n",
    "\n",
    "vocab = { token:integer for integer, token in enumerate(all_words)}\n",
    "#print(vocab)\n",
    "#print out the first 50\n",
    "for i, item in enumerate(vocab.items()):\n",
    "    print(item)\n",
    "    if i > 50:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a6c6bcaa-daa0-47e1-b7ab-77047965460d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# basic tokenizer class\n",
    "class SimpleTokenizerV1:\n",
    "    def __init__(self, vocab):\n",
    "        self.str_to_int = vocab #A\n",
    "        self.int_to_str = { i:s for s, i in vocab.items() }\n",
    "\n",
    "    def encode(self, text): #C\n",
    "        preprocessed = re.split(r'([,.?_!\"()\\']|--|\\s)', text)\n",
    "        preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
    "        ids = [self.str_to_int[s] for s in preprocessed]\n",
    "        return ids\n",
    "\n",
    "    def decode(self, ids): #D\n",
    "        text = \" \".join([self.int_to_str[i] for i in ids])\n",
    "        text = re.sub(r'\\s+([,.?!\"()\\'])', r'\\1', text) #E\n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "47cf9a43-eb0c-410e-97cd-9a1ae524b78d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[881, 10, 1395, 10, 456, 954, 2324, 3090, 1826, 3090, 615, 10]\n",
      "Yes, dear, I am obliged to go to Nuremberg,\n",
      "3372\n",
      "('”', 3367)\n",
      "('•', 3368)\n",
      "('\\ufeffThe', 3369)\n",
      "('<|endoftext|>', 3370)\n",
      "('<|unk|>', 3371)\n"
     ]
    }
   ],
   "source": [
    "# start using the tokenizer\n",
    "tokenizer = SimpleTokenizerV1(vocab)\n",
    "text = \"\"\"Yes, dear, I am obliged to go to Nuremberg,\"\"\"\n",
    "ids = tokenizer.encode(text)\n",
    "print(ids)\n",
    "\n",
    "print(tokenizer.decode(ids))\n",
    "\n",
    "all_tokens = sorted(list(set(preprocessed)))\n",
    "all_tokens.extend([\"<|endoftext|>\", \"<|unk|>\"])\n",
    "vocab = { token:integer for integer, token in enumerate(all_tokens) }\n",
    "\n",
    "print(len(vocab.items()))\n",
    "\n",
    "for i, item in enumerate(list(vocab.items())[-5:]):\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "02095d21-7265-42b2-85f9-7d7f1744ea9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# updated tokenizer\n",
    "from typing import List\n",
    "unknown_token = \"<|unk|>\"\n",
    "eot_token = \"<|endoftext|>\"\n",
    "\n",
    "class SimpleTokenizerV2:\n",
    "    def __init__(self, vocab: list) -> None:\n",
    "        self.str_to_int = vocab\n",
    "        self.int_to_str = { i:s for s, i in vocab.items() }\n",
    "\n",
    "    def encode(self, text: str) -> List:\n",
    "        preprocessed = re.split(r'([,.?_!\"()\\']|--|\\s)', text)\n",
    "        preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
    "        preprocessed = [item if item in self.str_to_int #A\n",
    "                        else unknown_token for item in preprocessed]\n",
    "        ids = [self.str_to_int[s] for s in preprocessed]\n",
    "        return ids\n",
    "\n",
    "    def decode(self, ids: List) -> str:\n",
    "        text = \" \".join([self.int_to_str[i] for i in ids])\n",
    "        text = re.sub(r'\\s+([,.?!\"()\\'])', r'\\1', text) #B\n",
    "        return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f5ed7736-bd3b-4b93-a3af-167e1e3513b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, do you like tea? <|endoftext|> In the sunlit terraces of the palace.\n",
      "[3371, 10, 1486, 3352, 2131, 3017, 121, 3370, 474, 3039, 3371, 3371, 2334, 3039, 2402, 13]\n",
      "<|unk|>, do you like tea? <|endoftext|> In the <|unk|> <|unk|> of the palace.\n"
     ]
    }
   ],
   "source": [
    "text1 = \"Hello, do you like tea?\"\n",
    "text2 = \"In the sunlit terraces of the palace.\"\n",
    "text = f\" {eot_token} \".join((text1, text2))\n",
    "print(text)\n",
    "tokenizer = SimpleTokenizerV2(vocab)\n",
    "print(tokenizer.encode(text))\n",
    "print(tokenizer.decode(tokenizer.encode(text)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "daec9d47-467e-4fae-837f-fc4be322218b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tiktoken version:  0.9.0\n"
     ]
    }
   ],
   "source": [
    "# now we can just use a python lib of a more sophisticated tokenizer\n",
    "# poetry add tiktoken, and checking the version here.\n",
    "from importlib.metadata import version\n",
    "import tiktoken\n",
    "print(\"tiktoken version: \", version(\"tiktoken\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1bf95182-e655-4064-8229-611e9493fe02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15496, 11, 466, 345, 588, 8887, 30, 220, 50256, 554, 262, 4252, 18250, 1059, 430]\n",
      "Hello, do you like tea? <|endoftext|> In the sunlit terra\n"
     ]
    }
   ],
   "source": [
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "text = f\"Hello, do you like tea? {eot_token} In the sunlit terra\"\n",
    "integers = tokenizer.encode(text, allowed_special={eot_token})\n",
    "print(integers)\n",
    "strings = tokenizer.decode(integers)\n",
    "print(strings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "79382fe9-1f63-4605-9c3f-bc36c6879250",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26663\n",
      "x: [351, 2048, 645, 8733]\n",
      "\ty: [2048, 645, 8733, 198]\n",
      "[351] ----> 2048\n",
      "[351, 2048] ----> 645\n",
      "[351, 2048, 645] ----> 8733\n",
      "[351, 2048, 645, 8733] ----> 198\n",
      " with ---->  almost\n",
      " with almost ---->  no\n",
      " with almost no ---->  restrictions\n",
      " with almost no restrictions ----> \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Tokenize the input text so we can muck around with a sliding window\n",
    "enc_text = tokenizer.encode(raw_text)\n",
    "print(len(enc_text))\n",
    "\n",
    "# remove some 50 entries to make things interesting\n",
    "enc_sample = enc_text[50:]\n",
    "\n",
    "# creating the sliding window (x / y shifted by 1)\n",
    "# to create input/target pairs for next word prediction\n",
    "context_size = 4\n",
    "\n",
    "x = enc_sample[:context_size]\n",
    "y = enc_sample[1:context_size+1]\n",
    "\n",
    "print(f\"x: {x}\")\n",
    "print(f\"\\ty: {y}\")\n",
    "\n",
    "for i in range(1, context_size+1):\n",
    "    context = enc_sample[:i]\n",
    "    desired = enc_sample[i]\n",
    "    # everything to the left represents the input the LLM would recieve\n",
    "    print(context, \"---->\", desired)\n",
    "\n",
    "\n",
    "# same test but converted ids back to strings\n",
    "for i in range(1, context_size+1):\n",
    "    context = enc_sample[:i]\n",
    "    desired = enc_sample[i]\n",
    "    print(tokenizer.decode(context), \"---->\", tokenizer.decode([desired]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "45d38194-4fc5-4714-a822-a80a83e65464",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ContextProp', 'PropModule', '__allow_nonbracketed_mutation', '__allow_nonbracketed_mutation_flag', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__path__', '__spec__', 'contextmanager', 'cpu', 'cuda', 'cudnn', 'cusparselt', 'disable_global_flags', 'flags_frozen', 'kleidiai', 'mha', 'mkl', 'mkldnn', 'mps', 'nnpack', 'openmp', 'quantized', 'types']\n",
      "tensor(1)\n",
      "tensor([1, 2, 3])\n",
      "tensor([[1, 2],\n",
      "        [3, 4]])\n",
      "tensor([[[1, 2],\n",
      "         [3, 4]],\n",
      "\n",
      "        [[5, 6],\n",
      "         [7, 8]]])\n"
     ]
    }
   ],
   "source": [
    "# installed torch rocm version\n",
    "# poetry run pip3 install torch --index-url https://download.pytorch.org/whl/rocm6.2.4\n",
    "import torch\n",
    "print(dir(torch.backends))\n",
    "torch.__version__\n",
    "tensor0d = torch.tensor(1) #A\n",
    "tensor1d = torch.tensor([1, 2, 3]) #B\n",
    "tensor2d = torch.tensor([[1, 2], [3, 4]]) #C\n",
    "tensor3d = torch.tensor([[[1, 2], [3, 4]], [[5, 6], [7, 8]]]) #D\n",
    "print(tensor0d)\n",
    "print(tensor1d)\n",
    "print(tensor2d)\n",
    "print(tensor3d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "49cb3c97-26b8-416e-b452-45a418286ead",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# Dataset class\n",
    "class GPTDatasetV1(Dataset):\n",
    "    def __init__(self, txt: str, tokenizer, max_length:int, stride:int) -> None:\n",
    "        # stride is how many positions we shift the sliding window, if stride == window size\n",
    "        # we can eliminate the overlap in batches\n",
    "        self.tokenizer = tokenizer\n",
    "        self.input_ids = []\n",
    "        self.target_ids = []\n",
    "\n",
    "        token_ids = tokenizer.encode(txt) # A\n",
    "\n",
    "        for i in range(0, len(token_ids) - max_length, stride): # B\n",
    "            input_chunk = token_ids[i:i + max_length]\n",
    "            target_chunk = token_ids[i + 1: i + max_length + 1]\n",
    "            self.input_ids.append(torch.tensor(input_chunk))\n",
    "            self.target_ids.append(torch.tensor(target_chunk))\n",
    "            \n",
    "    def __len__(self): #C\n",
    "        return len(self.input_ids)\n",
    "\n",
    "    def __getitem__(self, idx): # D\n",
    "        return self.input_ids[idx], self.target_ids[idx]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "99853128-0e04-4ed8-bbb2-a6266738ca26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[171, 119, 123, 464]]), tensor([[ 119,  123,  464, 4935]])]\n",
      "[tensor([[ 119,  123,  464, 4935]]), tensor([[  123,   464,  4935, 20336]])]\n"
     ]
    }
   ],
   "source": [
    "def create_dataloader_v1(txt, batch_size=4, max_length=256, stride=128, shuffle=True, drop_last=True):\n",
    "    tokenizer = tiktoken.get_encoding(\"gpt2\") # A\n",
    "    dataset = GPTDatasetV1(txt, tokenizer, max_length, stride) # B\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=shuffle, drop_last=drop_last)\n",
    "    return dataloader\n",
    "\n",
    "dataloader = create_dataloader_v1(raw_text, batch_size=1, max_length=4, stride=1, shuffle=False)\n",
    "data_iter = iter(dataloader) # A\n",
    "first_batch = next(data_iter)\n",
    "print(first_batch)\n",
    "second_batch = next(data_iter)\n",
    "print(second_batch)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8384bfb4-39d0-4599-b100-c8bec6aec04d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inputs:\n",
      " tensor([[  171,   119,   123,   464],\n",
      "        [ 4935, 20336, 46566,   286],\n",
      "        [ 1675,   399,   495, 47369],\n",
      "        [  290,  5157,    25,   317],\n",
      "        [ 7430,   338, 22770,   198],\n",
      "        [  220,   220,   220,   220],\n",
      "        [  198,  1212, 47179,   318],\n",
      "        [  329,   262,   779,   286]])\n",
      "\n",
      "Targets:\n",
      " tensor([[  119,   123,   464,  4935],\n",
      "        [20336, 46566,   286,  1675],\n",
      "        [  399,   495, 47369,   290],\n",
      "        [ 5157,    25,   317,  7430],\n",
      "        [  338, 22770,   198,   220],\n",
      "        [  220,   220,   220,   198],\n",
      "        [ 1212, 47179,   318,   329],\n",
      "        [  262,   779,   286,  2687]])\n"
     ]
    }
   ],
   "source": [
    "dataloader = create_dataloader_v1(raw_text, batch_size=8, max_length=4, stride=4, shuffle=False)\n",
    "data_iter = iter(dataloader)\n",
    "inputs, targets = next(data_iter)\n",
    "# without overlap since we are doing max 4 / stride 4\n",
    "print(\"Inputs:\\n\", inputs)\n",
    "print(\"\\nTargets:\\n\", targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f69a69d8-dfd1-4ce8-8f6a-8342f8bbd70f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([-0.0898]),)\n",
      "(tensor([-0.0817]),)\n",
      "tensor([-0.0898])\n",
      "tensor([-0.0817])\n"
     ]
    }
   ],
   "source": [
    "# trying to figure out what autograd is, to do backpropagation\n",
    "# so that we can compute the gradient of the loss \n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import grad\n",
    "y = torch.tensor([1.0]) # target\n",
    "x1 = torch.tensor([1.1])   # input\n",
    "w1 = torch.tensor([2.2], requires_grad=True) #trainable weight param\n",
    "b = torch.tensor([0.0], requires_grad=True)  #trainable bias unit\n",
    "z = x1 * w1 + b\n",
    "a = torch.sigmoid(z)\n",
    "loss = F.binary_cross_entropy(a, y)\n",
    "grad_L_w1 = grad(loss, w1, retain_graph=True) #A\n",
    "grad_L_b = grad(loss, b, retain_graph=True)\n",
    "#Let's show the resulting values of the loss with respect to the model's\n",
    "#parameters:\n",
    "print(grad_L_w1)\n",
    "print(grad_L_b)\n",
    "\n",
    "# or pytorch just has this built in as .backward()\n",
    "loss.backward()\n",
    "print(w1.grad)\n",
    "print(b.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3ac02f92-0055-4383-82d7-dcf4cc97a31f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[ 0.3374, -0.1778, -0.1690],\n",
      "        [ 0.9178,  1.5810,  1.3010],\n",
      "        [ 1.2753, -0.2010, -0.1606],\n",
      "        [-0.4015,  0.9666, -1.1481],\n",
      "        [-1.1589,  0.3255, -0.6315],\n",
      "        [-2.8400, -0.7849, -1.4096]], requires_grad=True)\n",
      "tensor([[-0.4015,  0.9666, -1.1481]], grad_fn=<EmbeddingBackward0>)\n",
      "tensor([[ 1.2753, -0.2010, -0.1606],\n",
      "        [-0.4015,  0.9666, -1.1481],\n",
      "        [-2.8400, -0.7849, -1.4096],\n",
      "        [ 0.9178,  1.5810,  1.3010]], grad_fn=<EmbeddingBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# setup a very small embedding space to muck around\n",
    "input_ids = torch.tensor([2, 3, 5, 1])\n",
    "vocab_size = 6 # BPE has 50k words (the gpt2 encoder use use above) BytePair Encoding (breaks down words to smaller tokens to handle unknowns)\n",
    "output_dim = 3  # GPT-3 has an embedding size 12,288 dimentions for reference\n",
    "torch.manual_seed(123) #set seed manually for reproducible results\n",
    "embedding_layer = torch.nn.Embedding(vocab_size, output_dim)\n",
    "print(embedding_layer.weight)\n",
    "# apply it to a token to get the embedding vector\n",
    "print(embedding_layer(torch.tensor([3])))\n",
    "# apply it to all our ids\n",
    "print(embedding_layer(input_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fd2c4d3c-7bf2-408a-aae3-fa090850da89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token IDs:\n",
      " tensor([[  171,   119,   123,   464],\n",
      "        [ 4935, 20336, 46566,   286],\n",
      "        [ 1675,   399,   495, 47369],\n",
      "        [  290,  5157,    25,   317],\n",
      "        [ 7430,   338, 22770,   198],\n",
      "        [  220,   220,   220,   220],\n",
      "        [  198,  1212, 47179,   318],\n",
      "        [  329,   262,   779,   286]])\n",
      "\n",
      "Inputs shape:\n",
      " torch.Size([8, 4])\n",
      "torch.Size([8, 4, 256])\n",
      "torch.Size([4, 256])\n",
      "torch.Size([8, 4, 256])\n"
     ]
    }
   ],
   "source": [
    "# Now we can start creating the token \"embeddings\" / vectors\n",
    "\n",
    "#create more realistic dimensions\n",
    "output_dim = 256\n",
    "vocab_size = 50257\n",
    "token_embedding_layer = torch.nn.Embedding(vocab_size, output_dim)\n",
    "\n",
    "max_length = 4\n",
    "dataloader = create_dataloader_v1(raw_text, batch_size=8, max_length=max_length, stride=max_length, shuffle=False)\n",
    "data_iter = iter(dataloader)\n",
    "inputs, targets = next(data_iter)\n",
    "print(\"Token IDs:\\n\", inputs)\n",
    "print(\"\\nInputs shape:\\n\", inputs.shape)\n",
    "\n",
    "token_embeddings = token_embedding_layer(inputs)\n",
    "print(token_embeddings.shape)\n",
    "\n",
    "# absolute embedding approach\n",
    "context_length = max_length\n",
    "pos_embedding_layer = torch.nn.Embedding(context_length, output_dim)\n",
    "pos_embeddings = pos_embedding_layer(torch.arange(context_length))\n",
    "print(pos_embeddings.shape)\n",
    "\n",
    "input_embeddings = token_embeddings + pos_embeddings\n",
    "print(input_embeddings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ed00e675-0cac-49a8-8db8-0b54af8d93db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.9544, 1.4950, 1.4754, 0.8434, 0.7070, 1.0865])\n",
      "tensor(0.9544)\n",
      "tensor(0.9544)\n",
      "Attention weights: tensor([0.1455, 0.2278, 0.2249, 0.1285, 0.1077, 0.1656])\n",
      "Sum: tensor(1.0000)\n",
      "Attention weights: tensor([0.1385, 0.2379, 0.2333, 0.1240, 0.1082, 0.1581])\n",
      "Sum: tensor(1.)\n",
      "Attention weights: tensor([0.1385, 0.2379, 0.2333, 0.1240, 0.1082, 0.1581])\n",
      "Sum: tensor(1.)\n",
      "tensor([0.4419, 0.6515, 0.5683])\n",
      "tensor([[0.9995, 0.9544, 0.9422, 0.4753, 0.4576, 0.6310],\n",
      "        [0.9544, 1.4950, 1.4754, 0.8434, 0.7070, 1.0865],\n",
      "        [0.9422, 1.4754, 1.4570, 0.8296, 0.7154, 1.0605],\n",
      "        [0.4753, 0.8434, 0.8296, 0.4937, 0.3474, 0.6565],\n",
      "        [0.4576, 0.7070, 0.7154, 0.3474, 0.6654, 0.2935],\n",
      "        [0.6310, 1.0865, 1.0605, 0.6565, 0.2935, 0.9450]])\n"
     ]
    }
   ],
   "source": [
    "# starting with attention\n",
    "inputs = torch.tensor(\n",
    "[[0.43, 0.15, 0.89], # Your (x^1)\n",
    "[0.55, 0.87, 0.66], # journey (x^2)\n",
    "[0.57, 0.85, 0.64], # starts (x^3)\n",
    "[0.22, 0.58, 0.33], # with (x^4)\n",
    "[0.77, 0.25, 0.10], # one (x^5)\n",
    "[0.05, 0.80, 0.55]] # step (x^6)\n",
    ")\n",
    "\n",
    "query = inputs[1] #A\n",
    "attn_scores_2 = torch.empty(inputs.shape[0])\n",
    "for i, x_i in enumerate(inputs):\n",
    "    attn_scores_2[i] = torch.dot(x_i, query)\n",
    "print(attn_scores_2)\n",
    "\n",
    "# dot product\n",
    "res = 0.\n",
    "for idx, element in enumerate(inputs[0]):\n",
    "    res += inputs[0][idx] * query[idx]\n",
    "print(res)\n",
    "print(torch.dot(inputs[0], query))\n",
    "# higher dot.product is the more similar the vectors\n",
    "\n",
    "attn_weights_2_tmp = attn_scores_2 / attn_scores_2.sum() # normalization of the results\n",
    "print(\"Attention weights:\", attn_weights_2_tmp)\n",
    "print(\"Sum:\", attn_weights_2_tmp.sum())  # do normalization so all the sum == 1.\n",
    "\n",
    "# normalization usually done with softmax, here's a naive implementation\n",
    "\n",
    "def softmax_naive(x):\n",
    "    return torch.exp(x) / torch.exp(x).sum(dim=0)\n",
    "attn_weights_2_naive = softmax_naive(attn_scores_2)\n",
    "print(\"Attention weights:\", attn_weights_2_naive)\n",
    "print(\"Sum:\", attn_weights_2_naive.sum())\n",
    "\n",
    "# pytorch also supports this directly\n",
    "attn_weights_2 = torch.softmax(attn_scores_2, dim=0)\n",
    "print(\"Attention weights:\", attn_weights_2)\n",
    "print(\"Sum:\", attn_weights_2.sum())\n",
    "\n",
    "# now get the context vector\n",
    "query = inputs[1] # 2nd input token is the query\n",
    "context_vec_2 = torch.zeros(query.shape)\n",
    "for i,x_i in enumerate(inputs):\n",
    "    context_vec_2 += attn_weights_2[i]*x_i\n",
    "print(context_vec_2)\n",
    "\n",
    "# compute dot.product for all input pairs\n",
    "attn_scores = torch.empty(6, 6)\n",
    "for i, x_i in enumerate(inputs):\n",
    "    for j, x_j in enumerate(inputs):\n",
    "        attn_scores[i, j] = torch.dot(x_i, x_j)\n",
    "print(attn_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c2b4bc55-4bc6-4796-aa40-d218930da521",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.9995, 0.9544, 0.9422, 0.4753, 0.4576, 0.6310],\n",
      "        [0.9544, 1.4950, 1.4754, 0.8434, 0.7070, 1.0865],\n",
      "        [0.9422, 1.4754, 1.4570, 0.8296, 0.7154, 1.0605],\n",
      "        [0.4753, 0.8434, 0.8296, 0.4937, 0.3474, 0.6565],\n",
      "        [0.4576, 0.7070, 0.7154, 0.3474, 0.6654, 0.2935],\n",
      "        [0.6310, 1.0865, 1.0605, 0.6565, 0.2935, 0.9450]])\n",
      "tensor([[0.2098, 0.2006, 0.1981, 0.1242, 0.1220, 0.1452],\n",
      "        [0.1385, 0.2379, 0.2333, 0.1240, 0.1082, 0.1581],\n",
      "        [0.1390, 0.2369, 0.2326, 0.1242, 0.1108, 0.1565],\n",
      "        [0.1435, 0.2074, 0.2046, 0.1462, 0.1263, 0.1720],\n",
      "        [0.1526, 0.1958, 0.1975, 0.1367, 0.1879, 0.1295],\n",
      "        [0.1385, 0.2184, 0.2128, 0.1420, 0.0988, 0.1896]])\n",
      "Row 2 sum: 1.0\n",
      "All row sums: tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000])\n",
      "tensor([[0.4421, 0.5931, 0.5790],\n",
      "        [0.4419, 0.6515, 0.5683],\n",
      "        [0.4431, 0.6496, 0.5671],\n",
      "        [0.4304, 0.6298, 0.5510],\n",
      "        [0.4671, 0.5910, 0.5266],\n",
      "        [0.4177, 0.6503, 0.5645]])\n",
      "Previous 2nd context vector: tensor([0.4419, 0.6515, 0.5683])\n"
     ]
    }
   ],
   "source": [
    "# actual attention here\n",
    "\n",
    "\n",
    "# 1\n",
    "attn_scores = inputs @ inputs.T # matrix mult with numpy operator\n",
    "print(attn_scores)\n",
    "\n",
    "#2\n",
    "attn_weights = torch.softmax(attn_scores, dim=1)\n",
    "print(attn_weights)\n",
    "\n",
    "# test to make sure normalization still == 1\n",
    "row_2_sum = sum([0.1385, 0.2379, 0.2333, 0.1240, 0.1082, 0.1581])\n",
    "print(\"Row 2 sum:\", row_2_sum)\n",
    "print(\"All row sums:\", attn_weights.sum(dim=1))\n",
    "\n",
    "# 3\n",
    "# compute all context vectors \n",
    "all_context_vecs = attn_weights @ inputs\n",
    "print(all_context_vecs)\n",
    "# just validating result\n",
    "print(\"Previous 2nd context vector:\", context_vec_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "212a7ef7-bafc-48e6-b158-4282788f7b1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.4306, 1.4551])\n",
      "keys.shape: torch.Size([6, 2])\n",
      "values.shape: torch.Size([6, 2])\n",
      "tensor(1.8524)\n",
      "tensor([1.2705, 1.8524, 1.8111, 1.0795, 0.5577, 1.5440])\n",
      "tensor([0.1500, 0.2264, 0.2199, 0.1311, 0.0906, 0.1820])\n",
      "tensor([0.3061, 0.8210])\n"
     ]
    }
   ],
   "source": [
    "# attention with trainable weights\n",
    "x_2 = inputs[1] #A\n",
    "d_in = inputs.shape[1] #B\n",
    "d_out = 2 #C\n",
    "\n",
    "torch.manual_seed(123)\n",
    "W_query = torch.nn.Parameter(torch.rand(d_in, d_out), requires_grad=False) # would be True if we were doing training\n",
    "W_key = torch.nn.Parameter(torch.rand(d_in, d_out), requires_grad=False)\n",
    "W_value = torch.nn.Parameter(torch.rand(d_in, d_out), requires_grad=False)\n",
    "\n",
    "\n",
    "# Weight paramaters that are trained (learned coefficients that define the networks connections), \n",
    "# which are different from the contextual attention\"weights\" in previous sections\n",
    "query_2 = x_2 @ W_query\n",
    "key_2 = x_2 @ W_key\n",
    "value_2 = x_2 @ W_value\n",
    "print(query_2)\n",
    "\n",
    "keys = inputs @ W_key\n",
    "values = inputs @ W_value\n",
    "print(\"keys.shape:\", keys.shape)\n",
    "print(\"values.shape:\", values.shape)\n",
    "\n",
    "keys_2 = keys[1] #A\n",
    "attn_score_22 = query_2.dot(keys_2)\n",
    "print(attn_score_22)\n",
    "\n",
    "attn_scores_2 = query_2 @ keys.T # All attention scores for given query\n",
    "print(attn_scores_2)\n",
    "\n",
    "# scaling by the square root of the embedding dimension is the reason why\n",
    "# this self-attention mechanism is also called scaled-dot product attention\n",
    "d_k = keys.shape[-1]\n",
    "attn_weights_2 = torch.softmax(attn_scores_2 / d_k**0.5, dim=-1) # **0.5 == sqrt shorthand\n",
    "print(attn_weights_2)\n",
    "\n",
    "# calc context_vect_2\n",
    "context_vec_2 = attn_weights_2 @ values\n",
    "print(context_vec_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7cae3b8a-e76a-411b-bac3-5511e3c73a65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2996, 0.8053],\n",
      "        [0.3061, 0.8210],\n",
      "        [0.3058, 0.8203],\n",
      "        [0.2948, 0.7939],\n",
      "        [0.2927, 0.7891],\n",
      "        [0.2990, 0.8040]], grad_fn=<MmBackward0>)\n",
      "tensor([[-0.0858,  0.0607],\n",
      "        [-0.0921,  0.0578],\n",
      "        [-0.0912,  0.0588],\n",
      "        [-0.0885,  0.0572],\n",
      "        [-0.0689,  0.0845],\n",
      "        [-0.0973,  0.0461]], grad_fn=<MmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# self attention class\n",
    "\n",
    "import torch.nn as nn\n",
    "\n",
    "class SelfAttention_v1(nn.Module):\n",
    "    def __init__(self, d_in, d_out):\n",
    "        super().__init__()\n",
    "        self.d_out = d_out\n",
    "        self.W_query = nn.Parameter(torch.rand(d_in, d_out))\n",
    "        self.W_key = nn.Parameter(torch.rand(d_in, d_out))\n",
    "        self.W_value = nn.Parameter(torch.rand(d_in, d_out))\n",
    "\n",
    "    def forward(self, x):\n",
    "        keys = x @ self.W_key\n",
    "        queries = x @ self.W_query\n",
    "        values = x @ self.W_value\n",
    "        attention_scores = queries @ keys.T # omega\n",
    "        attention_weights = torch.softmax(attention_scores/ keys.shape[-1]**0.5, dim=-1)\n",
    "        context_vector = attention_weights @ values\n",
    "        return context_vector\n",
    "\n",
    "torch.manual_seed(123)\n",
    "sa_v1 = SelfAttention_v1(d_in, d_out)\n",
    "print(sa_v1(inputs))\n",
    "\n",
    "# better optimized with some pytorch builtin\n",
    "\n",
    "class SelfAttention_v2(nn.Module):\n",
    "    def __init__(self, d_in, d_out, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        self.d_out = d_out\n",
    "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        keys = self.W_key(x)\n",
    "        queries = self.W_query(x)\n",
    "        values = self.W_value(x)\n",
    "        attention_scores = queries @ keys.T\n",
    "        attention_weights = torch.softmax(attention_scores / keys.shape[-1]**0.5, dim=-1)\n",
    "        context_vector = attn_weights @ values\n",
    "        return context_vector\n",
    "\n",
    "torch.manual_seed(789)\n",
    "sa_v2 = SelfAttention_v2(d_in, d_out)\n",
    "print(sa_v2(inputs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "27fac894-b4a2-466f-a953-99537163ff63",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  bonus stuff from chap3\n",
    "# Suppose we have the following 3 training examples,\n",
    "# which may represent token IDs in a LLM context\n",
    "idx = torch.tensor([2, 3, 1])\n",
    "\n",
    "# The number of rows in the embedding matrix can be determined\n",
    "# by obtaining the largest token ID + 1.\n",
    "# If the highest token ID is 3, then we want 4 rows, for the possible\n",
    "# token IDs 0, 1, 2, 3\n",
    "num_idx = max(idx)+1\n",
    "\n",
    "# The desired embedding dimension is a hyperparameter\n",
    "out_dim = 5\n",
    "# We use the random seed for reproducibility since\n",
    "# weights in the embedding layer are initialized with\n",
    "# small random values\n",
    "torch.manual_seed(123)\n",
    "\n",
    "embedding = torch.nn.Embedding(num_idx, out_dim)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c29d67b7-b1b5-4dbb-afd2-81c1d17cc999",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[ 0.3374, -0.1778, -0.3035, -0.5880,  1.5810],\n",
       "        [ 1.3010,  1.2753, -0.2010, -0.1606, -0.4015],\n",
       "        [ 0.6957, -1.8061, -1.1589,  0.3255, -0.6315],\n",
       "        [-2.8400, -0.7849, -1.4096, -0.4076,  0.7953]], requires_grad=True)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d05b1d83-8043-4721-94c1-377b0180f75d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.3010,  1.2753, -0.2010, -0.1606, -0.4015]],\n",
       "       grad_fn=<EmbeddingBackward0>)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding(torch.tensor([1]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2dc724e3-09fd-42e3-a6a5-4eea6208fb9c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.6957, -1.8061, -1.1589,  0.3255, -0.6315]],\n",
       "       grad_fn=<EmbeddingBackward0>)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding(torch.tensor([2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "cc68ffe0-ea2c-485a-983a-bdab1c43869b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.6957, -1.8061, -1.1589,  0.3255, -0.6315],\n",
       "        [-2.8400, -0.7849, -1.4096, -0.4076,  0.7953],\n",
       "        [ 1.3010,  1.2753, -0.2010, -0.1606, -0.4015]],\n",
       "       grad_fn=<EmbeddingBackward0>)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx = torch.tensor([2, 3, 1])\n",
    "embedding(idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "65050652-41cf-4289-b514-dfec48d09701",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 0, 1, 0],\n",
       "        [0, 0, 0, 1],\n",
       "        [0, 1, 0, 0]])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "onehot = torch.nn.functional.one_hot(idx)\n",
    "onehot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "83f33a32-9b8b-4d67-a62f-accf2c9eb9c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[-0.2039,  0.0166, -0.2483,  0.1886],\n",
       "        [-0.4260,  0.3665, -0.3634, -0.3975],\n",
       "        [-0.3159,  0.2264, -0.1847,  0.1871],\n",
       "        [-0.4244, -0.3034, -0.1836, -0.0983],\n",
       "        [-0.3814,  0.3274, -0.1179,  0.1605]], requires_grad=True)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "linear = torch.nn.Linear(num_idx, out_dim, bias=False)\n",
    "linear.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "fd66f5d4-9a73-449a-9f7e-f708b0f012c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.6957, -1.8061, -1.1589,  0.3255, -0.6315],\n",
       "        [-2.8400, -0.7849, -1.4096, -0.4076,  0.7953],\n",
       "        [ 1.3010,  1.2753, -0.2010, -0.1606, -0.4015]], grad_fn=<MmBackward0>)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linear.weight = torch.nn.Parameter(embedding.weight.T)\n",
    "linear(onehot.float())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5e93cd3f-5244-498f-b442-ed5fdc20b197",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.6957, -1.8061, -1.1589,  0.3255, -0.6315],\n",
       "        [-2.8400, -0.7849, -1.4096, -0.4076,  0.7953],\n",
       "        [ 1.3010,  1.2753, -0.2010, -0.1606, -0.4015]],\n",
       "       grad_fn=<EmbeddingBackward0>)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding(idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "76fa7c69-5857-4a17-a8e5-58ba8ad5e885",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1921, 0.1646, 0.1652, 0.1550, 0.1721, 0.1510],\n",
      "        [0.2041, 0.1659, 0.1662, 0.1496, 0.1665, 0.1477],\n",
      "        [0.2036, 0.1659, 0.1662, 0.1498, 0.1664, 0.1480],\n",
      "        [0.1869, 0.1667, 0.1668, 0.1571, 0.1661, 0.1564],\n",
      "        [0.1830, 0.1669, 0.1670, 0.1588, 0.1658, 0.1585],\n",
      "        [0.1935, 0.1663, 0.1666, 0.1542, 0.1666, 0.1529]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0.],\n",
      "        [1., 1., 1., 1., 1., 1.]])\n",
      "tensor([[0.1921, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2041, 0.1659, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2036, 0.1659, 0.1662, 0.0000, 0.0000, 0.0000],\n",
      "        [0.1869, 0.1667, 0.1668, 0.1571, 0.0000, 0.0000],\n",
      "        [0.1830, 0.1669, 0.1670, 0.1588, 0.1658, 0.0000],\n",
      "        [0.1935, 0.1663, 0.1666, 0.1542, 0.1666, 0.1529]],\n",
      "       grad_fn=<MulBackward0>)\n",
      "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.5517, 0.4483, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.3800, 0.3097, 0.3103, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2758, 0.2460, 0.2462, 0.2319, 0.0000, 0.0000],\n",
      "        [0.2175, 0.1983, 0.1984, 0.1888, 0.1971, 0.0000],\n",
      "        [0.1935, 0.1663, 0.1666, 0.1542, 0.1666, 0.1529]],\n",
      "       grad_fn=<DivBackward0>)\n",
      "tensor([[0.2899,   -inf,   -inf,   -inf,   -inf,   -inf],\n",
      "        [0.4656, 0.1723,   -inf,   -inf,   -inf,   -inf],\n",
      "        [0.4594, 0.1703, 0.1731,   -inf,   -inf,   -inf],\n",
      "        [0.2642, 0.1024, 0.1036, 0.0186,   -inf,   -inf],\n",
      "        [0.2183, 0.0874, 0.0882, 0.0177, 0.0786,   -inf],\n",
      "        [0.3408, 0.1270, 0.1290, 0.0198, 0.1290, 0.0078]],\n",
      "       grad_fn=<MaskedFillBackward0>)\n",
      "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.5517, 0.4483, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.3800, 0.3097, 0.3103, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2758, 0.2460, 0.2462, 0.2319, 0.0000, 0.0000],\n",
      "        [0.2175, 0.1983, 0.1984, 0.1888, 0.1971, 0.0000],\n",
      "        [0.1935, 0.1663, 0.1666, 0.1542, 0.1666, 0.1529]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# causal *masked* attention only considers the previous and currents input in a seqenence\n",
    "\n",
    "queries = sa_v2.W_query(inputs) #A\n",
    "keys = sa_v2.W_key(inputs)\n",
    "attn_scores = queries @ keys.T\n",
    "attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=1)\n",
    "print(attn_weights)\n",
    "\n",
    "\n",
    "# create the mask with pytorch's tril\n",
    "context_length = attn_scores.shape[0]\n",
    "mask_simple = torch.tril(torch.ones(context_length, context_length))\n",
    "print(mask_simple)\n",
    "\n",
    "masked_simple = attn_weights*mask_simple\n",
    "print(masked_simple)\n",
    "\n",
    "# renorm\n",
    "\n",
    "row_sums = masked_simple.sum(dim=1, keepdim=True)\n",
    "masked_simple_norm = masked_simple / row_sums\n",
    "print(masked_simple_norm)\n",
    "\n",
    "\n",
    "# more pytorch optimizations\n",
    "mask = torch.triu(torch.ones(context_length, context_length), diagonal=1)\n",
    "masked = attn_scores.masked_fill(mask.bool(), -torch.inf)\n",
    "print(masked)\n",
    "attn_weights = torch.softmax(masked / keys.shape[-1]**0.5, dim=1)\n",
    "print(attn_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b4b3c755-e013-4e7e-a390-26ef825ad1a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[2., 2., 0., 2., 2., 0.],\n",
      "        [0., 0., 0., 2., 0., 2.],\n",
      "        [2., 2., 2., 2., 0., 2.],\n",
      "        [0., 2., 2., 0., 0., 2.],\n",
      "        [0., 2., 0., 2., 0., 2.],\n",
      "        [0., 2., 2., 2., 2., 0.]])\n",
      "tensor([[2.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.7599, 0.6194, 0.6206, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.4921, 0.4925, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.3966, 0.0000, 0.3775, 0.0000, 0.0000],\n",
      "        [0.0000, 0.3327, 0.3331, 0.3084, 0.3331, 0.0000]],\n",
      "       grad_fn=<MulBackward0>)\n",
      "torch.Size([2, 6, 3])\n"
     ]
    }
   ],
   "source": [
    "# Dropout (to reduce overfitting) only ever used \"during\" training\n",
    "# drop out 50% for playing around\n",
    "\n",
    "torch.manual_seed(123)\n",
    "dropout = torch.nn.Dropout(0.5) #A\n",
    "example = torch.ones(6, 6) #B\n",
    "print(dropout(example))\n",
    "\n",
    "# apply drop out mask to Attention Weights\n",
    "torch.manual_seed(123)\n",
    "print(dropout(attn_weights))\n",
    "\n",
    "batch = torch.stack((inputs, inputs), dim=0)\n",
    "print(batch.shape) #A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "33aae7c9-41c4-4546-a9c0-988a8ae599e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.8.0.dev20250324+rocm6.3\n",
      "cuda\n"
     ]
    }
   ],
   "source": [
    "# test / benchmarks\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(device)\n",
    "torch.set_float32_matmul_precision('high')  # optimization was spit out by a pytorch warning\n",
    "batch_size = 8\n",
    "context_len = 1024\n",
    "embed_dim = 768\n",
    "embeddings = torch.randn((batch_size, context_len, embed_dim), device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "1f7e4d7d-80dd-41d3-909b-635dc20f27ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "context_vecs.shape: torch.Size([2, 6, 2])\n"
     ]
    }
   ],
   "source": [
    "# causal attention class\n",
    "\n",
    "class CausalAttention(nn.Module):\n",
    "    def __init__(self, d_in, d_out, context_length, dropout, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        self.d_out = d_out\n",
    "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.dropout = nn.Dropout(dropout) #A\n",
    "        self.register_buffer('mask',torch.triu(torch.ones(context_length, context_length),diagonal=1)) #B\n",
    "        \n",
    "    def forward(self, x):\n",
    "        b, num_tokens, d_in = x.shape #C\n",
    "        keys = self.W_key(x)\n",
    "        queries = self.W_query(x)\n",
    "        values = self.W_value(x)\n",
    "        attn_scores = queries @ keys.transpose(1, 2) #C\n",
    "        attn_scores.masked_fill_( #D\n",
    "        self.mask.bool()[:num_tokens, :num_tokens], -torch.inf)\n",
    "        attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=1)\n",
    "        attn_weights = self.dropout(attn_weights)\n",
    "        context_vec = attn_weights @ values\n",
    "        return context_vec\n",
    "\n",
    "torch.manual_seed(123)\n",
    "context_length = batch.shape[1]\n",
    "ca = CausalAttention(d_in, d_out, context_length, 0.0)\n",
    "context_vecs = ca(batch)\n",
    "print(\"context_vecs.shape:\", context_vecs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "97caa615-0dc6-4581-ab6e-f8205adf7b35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 1024, 768])\n",
      "tensor([[[ 0.1953,  0.0474,  0.0461, -0.1178, -0.0132, -0.1352,  0.1551,\n",
      "           0.3136],\n",
      "         [ 0.3070,  0.0666,  0.0613, -0.1634,  0.0060, -0.1601,  0.2403,\n",
      "           0.2293],\n",
      "         [ 0.4460,  0.0834,  0.0748, -0.2205,  0.0352, -0.1956,  0.3491,\n",
      "           0.1259],\n",
      "         [ 0.5372,  0.0980,  0.1091, -0.2483,  0.0571, -0.2142,  0.4285,\n",
      "           0.0467],\n",
      "         [ 0.7105,  0.0580,  0.0806, -0.2893,  0.1456, -0.2980,  0.5875,\n",
      "          -0.0692],\n",
      "         [ 1.0696,  0.1872,  0.1998, -0.4781,  0.1586, -0.3189,  0.8263,\n",
      "          -0.3613]],\n",
      "\n",
      "        [[ 0.1953,  0.0474,  0.0461, -0.1178, -0.0132, -0.1352,  0.1551,\n",
      "           0.3136],\n",
      "         [ 0.3070,  0.0666,  0.0613, -0.1634,  0.0060, -0.1601,  0.2403,\n",
      "           0.2293],\n",
      "         [ 0.4460,  0.0834,  0.0748, -0.2205,  0.0352, -0.1956,  0.3491,\n",
      "           0.1259],\n",
      "         [ 0.5372,  0.0980,  0.1091, -0.2483,  0.0571, -0.2142,  0.4285,\n",
      "           0.0467],\n",
      "         [ 0.7105,  0.0580,  0.0806, -0.2893,  0.1456, -0.2980,  0.5875,\n",
      "          -0.0692],\n",
      "         [ 1.0696,  0.1872,  0.1998, -0.4781,  0.1586, -0.3189,  0.8263,\n",
      "          -0.3613]]], grad_fn=<ViewBackward0>)\n",
      "context_vecs.shape: torch.Size([2, 6, 8])\n"
     ]
    }
   ],
   "source": [
    "# Multi-head class\n",
    "\n",
    "class MultiHeadAttentionWrapper(nn.Module):\n",
    "    def __init__(self, d_in, d_out, context_length, dropout, num_heads, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList(\n",
    "            [CausalAttention(d_in, d_out, context_length, dropout, qkv_bias)\n",
    "             for _ in range(num_heads)]\n",
    "        )\n",
    "        self.out_proj = nn.Linear(d_out*num_heads, d_out*num_heads)\n",
    "\n",
    "    def forward(self, x):\n",
    "        context_vec = torch.cat([head(x) for head in self.heads], dim=-1)\n",
    "        return self.out_proj(context_vec)\n",
    "\n",
    "\n",
    "mh = MultiHeadAttentionWrapper(\n",
    "    d_in=embed_dim,\n",
    "    d_out=embed_dim//12,\n",
    "    context_length=context_len,\n",
    "    dropout=0.0,\n",
    "    num_heads=12,\n",
    "    qkv_bias=False\n",
    ").to(device)\n",
    "\n",
    "out = mh(embeddings)\n",
    "print(out.shape)\n",
    "\n",
    "torch.manual_seed(123)\n",
    "context_length = batch.shape[1] # This is the number of tokens\n",
    "d_in, d_out = 3, 2\n",
    "mh = MultiHeadAttentionWrapper(d_in, d_out, context_length, 0.0, num_heads=4)\n",
    "context_vecs = mh(batch)\n",
    "print(context_vecs)\n",
    "print(\"context_vecs.shape:\", context_vecs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ee79a41f-aa16-47fe-8c8c-6d1e335ceb14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 1024, 768])\n"
     ]
    }
   ],
   "source": [
    "# moved all the benchmarks to a separate file\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_in, d_out, context_length, dropout, num_heads, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        assert d_out % num_heads == 0, \"d_out must be divisible by num_heads\"\n",
    "\n",
    "        self.d_out = d_out\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = d_out // num_heads  # Reduce the projection dim to match desired output dim\n",
    "\n",
    "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.out_proj = nn.Linear(d_out, d_out)  # Linear layer to combine head outputs\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.register_buffer(\"mask\", torch.triu(torch.ones(context_length, context_length), diagonal=1))\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, num_tokens, d_in = x.shape\n",
    "\n",
    "        keys = self.W_key(x)  # Shape: (b, num_tokens, d_out)\n",
    "        queries = self.W_query(x)\n",
    "        values = self.W_value(x)\n",
    "\n",
    "        # We implicitly split the matrix by adding a `num_heads` dimension\n",
    "        # Unroll last dim: (b, num_tokens, d_out) -> (b, num_tokens, num_heads, head_dim)\n",
    "        keys = keys.view(b, num_tokens, self.num_heads, self.head_dim)\n",
    "        values = values.view(b, num_tokens, self.num_heads, self.head_dim)\n",
    "        queries = queries.view(b, num_tokens, self.num_heads, self.head_dim)\n",
    "\n",
    "        # Transpose: (b, num_tokens, num_heads, head_dim) -> (b, num_heads, num_tokens, head_dim)\n",
    "        keys = keys.transpose(1, 2)\n",
    "        queries = queries.transpose(1, 2)\n",
    "        values = values.transpose(1, 2)\n",
    "\n",
    "        # Compute scaled dot-product attention (aka self-attention) with a causal mask\n",
    "        attn_scores = queries @ keys.transpose(2, 3)  # Dot product for each head\n",
    "\n",
    "        # Original mask truncated to the number of tokens and converted to boolean\n",
    "        mask_bool = self.mask.bool()[:num_tokens, :num_tokens]\n",
    "\n",
    "        # Use the mask to fill attention scores\n",
    "        attn_scores.masked_fill_(mask_bool, -torch.inf)\n",
    "\n",
    "        attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=-1)\n",
    "        attn_weights = self.dropout(attn_weights)\n",
    "\n",
    "        # Shape: (b, num_tokens, num_heads, head_dim)\n",
    "        context_vec = (attn_weights @ values).transpose(1, 2)\n",
    "\n",
    "        # Combine heads, where self.d_out = self.num_heads * self.head_dim\n",
    "        context_vec = context_vec.contiguous().view(b, num_tokens, self.d_out)\n",
    "        context_vec = self.out_proj(context_vec)  # optional projection\n",
    "\n",
    "        return context_vec\n",
    "\n",
    "mha = MultiHeadAttention(\n",
    "    d_in=embed_dim,\n",
    "    d_out=embed_dim,\n",
    "    context_length=context_len,\n",
    "    dropout=0.0,\n",
    "    num_heads=12,\n",
    "    qkv_bias=False\n",
    ").to(device)\n",
    "\n",
    "out = mha(embeddings)\n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "27d6f6d9-48a7-41aa-a746-b1098198dd32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[1.3208, 1.1631, 1.2879],\n",
      "          [1.1631, 2.2150, 1.8424],\n",
      "          [1.2879, 1.8424, 2.0402]],\n",
      "\n",
      "         [[0.4391, 0.7003, 0.5903],\n",
      "          [0.7003, 1.3737, 1.0620],\n",
      "          [0.5903, 1.0620, 0.9912]]]])\n",
      "First head:\n",
      " tensor([[1.3208, 1.1631, 1.2879],\n",
      "        [1.1631, 2.2150, 1.8424],\n",
      "        [1.2879, 1.8424, 2.0402]])\n",
      "\n",
      "Second head:\n",
      " tensor([[0.4391, 0.7003, 0.5903],\n",
      "        [0.7003, 1.3737, 1.0620],\n",
      "        [0.5903, 1.0620, 0.9912]])\n",
      "tensor([[[0.3190, 0.4858],\n",
      "         [0.2943, 0.3897],\n",
      "         [0.2856, 0.3593],\n",
      "         [0.2693, 0.3873],\n",
      "         [0.2639, 0.3928],\n",
      "         [0.2575, 0.4028]],\n",
      "\n",
      "        [[0.3190, 0.4858],\n",
      "         [0.2943, 0.3897],\n",
      "         [0.2856, 0.3593],\n",
      "         [0.2693, 0.3873],\n",
      "         [0.2639, 0.3928],\n",
      "         [0.2575, 0.4028]]], grad_fn=<ViewBackward0>)\n",
      "context_vecs.shape: torch.Size([2, 6, 2])\n"
     ]
    }
   ],
   "source": [
    "a = torch.tensor([[[[0.2745, 0.6584, 0.2775, 0.8573], #A\n",
    "[0.8993, 0.0390, 0.9268, 0.7388],\n",
    "[0.7179, 0.7058, 0.9156, 0.4340]],\n",
    "[[0.0772, 0.3565, 0.1479, 0.5331],\n",
    "[0.4066, 0.2318, 0.4545, 0.9737],\n",
    "[0.4606, 0.5159, 0.4220, 0.5786]]]])\n",
    "print(a @ a.transpose(2, 3))\n",
    "\n",
    "first_head = a[0, 0, :, :]\n",
    "first_res = first_head @ first_head.T\n",
    "print(\"First head:\\n\", first_res)\n",
    "second_head = a[0, 1, :, :]\n",
    "second_res = second_head @ second_head.T\n",
    "print(\"\\nSecond head:\\n\", second_res)\n",
    "\n",
    "torch.manual_seed(123)\n",
    "batch_size, context_length, d_in = batch.shape\n",
    "d_out = 2\n",
    "mha = MultiHeadAttention(d_in, d_out, context_length, 0.0, num_heads=2)\n",
    "context_vecs = mha(batch)\n",
    "print(context_vecs)\n",
    "print(\"context_vecs.shape:\", context_vecs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "40bb6abd-2196-422f-8538-0e86aaf59c9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ch 4\n",
    "\n",
    "GPT_CONFIG_124M = {\n",
    "    \"vocab_size\": 50257, # Vocabulary size\n",
    "    \"context_length\": 1024, # Context length\n",
    "    \"emb_dim\": 768, # Embedding dimension\n",
    "    \"n_heads\": 12, # Number of attention heads\n",
    "    \"n_layers\": 12, # Number of layers\n",
    "    \"drop_rate\": 0.1, # Dropout rate\n",
    "    \"qkv_bias\": False # Query-Key-Value bias\n",
    "}\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "class DummyGPTModel(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.tok_emb = nn.Embedding(cfg[\"vocab_size\"], cfg[\"emb_dim\"])\n",
    "        self.pos_emb = nn.Embedding(cfg[\"context_length\"], cfg[\"emb_dim\"])\n",
    "        self.drop_emb = nn.Dropout(cfg[\"drop_rate\"])\n",
    "        self.trf_blocks = nn.Sequential(\n",
    "            *[DummyTransformerBlock(cfg) for _ in range(cfg[\"n_layers\"])]) #\n",
    "        self.final_norm = DummyLayerNorm(cfg[\"emb_dim\"]) #B\n",
    "        self.out_head = nn.Linear(\n",
    "            cfg[\"emb_dim\"], cfg[\"vocab_size\"], bias=False\n",
    "        )\n",
    "        \n",
    "    def forward(self, in_idx):\n",
    "        batch_size, seq_len = in_idx.shape\n",
    "        tok_embeds = self.tok_emb(in_idx)\n",
    "        pos_embeds = self.pos_emb(torch.arange(seq_len, device=in_idx.device))\n",
    "        x = tok_embeds + pos_embeds\n",
    "        x = self.drop_emb(x)\n",
    "        x = self.trf_blocks(x)\n",
    "        x = self.final_norm(x)\n",
    "        logits = self.out_head(x)\n",
    "        return logits\n",
    "              \n",
    "class DummyTransformerBlock(nn.Module): #C\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "              \n",
    "    def forward(self, x): #D\n",
    "        return x\n",
    "              \n",
    "class DummyLayerNorm(nn.Module): #E\n",
    "    def __init__(self, normalized_shape, eps=1e-5): #F\n",
    "        super().__init__()\n",
    "              \n",
    "    def forward(self, x):\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "59c3af33-9a34-48b4-ab85-cdc83db0d01e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[6109, 3626, 6100,  345],\n",
      "        [6109, 1110, 6622,  257]])\n",
      "Output shape: torch.Size([2, 4, 50257])\n",
      "tensor([[[-1.2034,  0.3201, -0.7130,  ..., -1.5548, -0.2390, -0.4667],\n",
      "         [-0.1192,  0.4539, -0.4432,  ...,  0.2392,  1.3469,  1.2430],\n",
      "         [ 0.5307,  1.6720, -0.4695,  ...,  1.1966,  0.0111,  0.5835],\n",
      "         [ 0.0139,  1.6755, -0.3388,  ...,  1.1586, -0.0435, -1.0400]],\n",
      "\n",
      "        [[-1.0908,  0.1798, -0.9484,  ..., -1.6047,  0.2439, -0.4530],\n",
      "         [-0.7860,  0.5581, -0.0610,  ...,  0.4835, -0.0077,  1.6621],\n",
      "         [ 0.3567,  1.2698, -0.6398,  ..., -0.0162, -0.1296,  0.3717],\n",
      "         [-0.2407, -0.7349, -0.5102,  ...,  2.0057, -0.3694,  0.1814]]],\n",
      "       grad_fn=<UnsafeViewBackward0>)\n",
      "tensor([[0.2260, 0.3470, 0.0000, 0.2216, 0.0000, 0.0000],\n",
      "        [0.2133, 0.2394, 0.0000, 0.5198, 0.3297, 0.0000]],\n",
      "       grad_fn=<ReluBackward0>)\n",
      "Mean:\n",
      " tensor([[0.1324],\n",
      "        [0.2170]], grad_fn=<MeanBackward1>)\n",
      "Variance:\n",
      " tensor([[0.0231],\n",
      "        [0.0398]], grad_fn=<VarBackward0>)\n",
      "Normalized layer outputs:\n",
      " tensor([[ 0.6159,  1.4126, -0.8719,  0.5872, -0.8719, -0.8719],\n",
      "        [-0.0189,  0.1121, -1.0876,  1.5173,  0.5647, -1.0876]],\n",
      "       grad_fn=<DivBackward0>)\n",
      "Mean:\n",
      " tensor([[9.9341e-09],\n",
      "        [1.9868e-08]], grad_fn=<MeanBackward1>)\n",
      "Variance:\n",
      " tensor([[1.0000],\n",
      "        [1.0000]], grad_fn=<VarBackward0>)\n",
      "Mean:\n",
      " tensor([[    0.0000],\n",
      "        [    0.0000]], grad_fn=<MeanBackward1>)\n",
      "Variance:\n",
      " tensor([[1.0000],\n",
      "        [1.0000]], grad_fn=<VarBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "batch = []\n",
    "txt1 = \"Every effort moves you\"\n",
    "txt2 = \"Every day holds a\"\n",
    "batch.append(torch.tensor(tokenizer.encode(txt1)))\n",
    "batch.append(torch.tensor(tokenizer.encode(txt2)))\n",
    "batch = torch.stack(batch, dim=0)\n",
    "print(batch)\n",
    "\n",
    "torch.manual_seed(123)\n",
    "model = DummyGPTModel(GPT_CONFIG_124M)\n",
    "logits = model(batch)\n",
    "print(\"Output shape:\", logits.shape)\n",
    "print(logits)\n",
    "\n",
    "\n",
    "torch.manual_seed(123)\n",
    "batch_example = torch.randn(2, 5) #A\n",
    "layer = nn.Sequential(nn.Linear(5, 6), nn.ReLU())\n",
    "out = layer(batch_example)\n",
    "print(out)\n",
    "\n",
    "mean = out.mean(dim=-1, keepdim=True)\n",
    "var = out.var(dim=-1, keepdim=True)\n",
    "print(\"Mean:\\n\", mean)\n",
    "print(\"Variance:\\n\", var)\n",
    "\n",
    "\n",
    "out_norm = (out - mean) / torch.sqrt(var)\n",
    "mean = out_norm.mean(dim=-1, keepdim=True)\n",
    "var = out_norm.var(dim=-1, keepdim=True)\n",
    "print(\"Normalized layer outputs:\\n\", out_norm)\n",
    "print(\"Mean:\\n\", mean)\n",
    "print(\"Variance:\\n\", var)\n",
    "\n",
    "torch.set_printoptions(sci_mode=False)\n",
    "print(\"Mean:\\n\", mean)\n",
    "print(\"Variance:\\n\", var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "34abe8e3-4201-43a3-aab8-80fced5eec58",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, emb_dim):\n",
    "        super().__init__()\n",
    "        self.eps = 1e-5\n",
    "        self.scale = nn.Parameter(torch.ones(emb_dim))\n",
    "        self.shift = nn.Parameter(torch.zeros(emb_dim))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        mean = x.mean(dim=-1, keepdim=True)\n",
    "        var = x.var(dim=-1, keepdim=True, unbiased=False)\n",
    "        norm_x = (x - mean) / torch.sqrt(var + self.eps)\n",
    "        return self.scale * norm_x + self.shift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "0e48fa8e-2193-4cac-b108-da726a3f5cd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean:\n",
      " tensor([[    -0.0000],\n",
      "        [     0.0000]], grad_fn=<MeanBackward1>)\n",
      "Variance:\n",
      " tensor([[1.0000],\n",
      "        [1.0000]], grad_fn=<VarBackward0>)\n"
     ]
    }
   ],
   "source": [
    "ln = LayerNorm(emb_dim=5)\n",
    "out_ln = ln(batch_example)\n",
    "mean = out_ln.mean(dim=-1, keepdim=True)\n",
    "var = out_ln.var(dim=-1, unbiased=False, keepdim=True)\n",
    "print(\"Mean:\\n\", mean)\n",
    "print(\"Variance:\\n\", var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "03ace1b1-7c09-449c-bbf5-3b614be6c1bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GELU(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return 0.5 * x * (1 + torch.tanh(\n",
    "            torch.sqrt(torch.tensor(2.0 / torch.pi)) *\n",
    "            (x + 0.044715 * torch.pow(x, 3))\n",
    "        ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "ea54c38a-92e3-4c16-909f-1e40e9507b2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x300 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "gelu, relu = GELU(), nn.ReLU()\n",
    "x = torch.linspace(-3, 3, 100) #A\n",
    "y_gelu, y_relu = gelu(x), relu(x)\n",
    "plt.figure(figsize=(8, 3))\n",
    "for i, (y, label) in enumerate(zip([y_gelu, y_relu], [\"GELU\", \"ReLU\"]), 1):\n",
    "    plt.subplot(1, 2, i)\n",
    "    plt.plot(x, y)\n",
    "    plt.title(f\"{label} activation function\")\n",
    "    plt.xlabel(\"x\")\n",
    "    plt.ylabel(f\"{label}(x)\")\n",
    "    plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "9c3f90b5-dccf-4176-a63e-a43e23706d84",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "        nn.Linear(cfg[\"emb_dim\"], 4 * cfg[\"emb_dim\"]),\n",
    "        GELU(),\n",
    "        nn.Linear(4 * cfg[\"emb_dim\"], cfg[\"emb_dim\"]),\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "6d8679b0-ccc5-4648-af28-a13c269b03f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3, 768])\n"
     ]
    }
   ],
   "source": [
    "ffn = FeedForward(GPT_CONFIG_124M)\n",
    "x = torch.rand(2, 3, 768) #A\n",
    "out = ffn(x)\n",
    "print(out.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "8c750636-7ac3-41f3-bb7f-b432cb02d801",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExampleDeepNeuralNetwork(nn.Module):\n",
    "    def __init__(self, layer_sizes, use_shortcut):\n",
    "        super().__init__()\n",
    "        self.use_shortcut = use_shortcut\n",
    "        self.layers = nn.ModuleList([\n",
    "        # Implement 5 layers\n",
    "            nn.Sequential(nn.Linear(layer_sizes[0], layer_sizes[1]), GELU()),\n",
    "            nn.Sequential(nn.Linear(layer_sizes[1], layer_sizes[2]), GELU()),\n",
    "            nn.Sequential(nn.Linear(layer_sizes[2], layer_sizes[3]), GELU()),\n",
    "            nn.Sequential(nn.Linear(layer_sizes[3], layer_sizes[4]), GELU()),\n",
    "            nn.Sequential(nn.Linear(layer_sizes[4], layer_sizes[5]), GELU())\n",
    "        ])\n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "        # Compute the output of the current layer\n",
    "            layer_output = layer(x)\n",
    "            # Check if shortcut can be applied\n",
    "            if self.use_shortcut and x.shape == layer_output.shape:\n",
    "                x = x + layer_output\n",
    "            else:\n",
    "                x = layer_output\n",
    "        return x\n",
    "\n",
    "def print_gradients(model, x):\n",
    "    # Forward pass\n",
    "    output = model(x)\n",
    "    target = torch.tensor([[0.]])\n",
    "\n",
    "    # Calculate loss based on how close the target\n",
    "    # and output are\n",
    "    loss = nn.MSELoss()\n",
    "    loss = loss(output, target)\n",
    "    \n",
    "    # Backward pass to calculate the gradients\n",
    "    loss.backward()\n",
    "\n",
    "    for name, param in model.named_parameters():\n",
    "        if 'weight' in name:\n",
    "            # Print the mean absolute gradient of the weights\n",
    "            print(f\"{name} has gradient mean of {param.grad.abs().mean().item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "aa0ca16e-d090-4442-9e12-d301c66b1497",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layers.0.0.weight has gradient mean of 0.00020173587836325169\n",
      "layers.1.0.weight has gradient mean of 0.00012011159560643137\n",
      "layers.2.0.weight has gradient mean of 0.0007152040489017963\n",
      "layers.3.0.weight has gradient mean of 0.0013988736318424344\n",
      "layers.4.0.weight has gradient mean of 0.005049645435065031\n",
      "layers.0.0.weight has gradient mean of 0.22169798612594604\n",
      "layers.1.0.weight has gradient mean of 0.20694111287593842\n",
      "layers.2.0.weight has gradient mean of 0.3289700150489807\n",
      "layers.3.0.weight has gradient mean of 0.26657330989837646\n",
      "layers.4.0.weight has gradient mean of 1.3258544206619263\n"
     ]
    }
   ],
   "source": [
    "layer_sizes = [3, 3, 3, 3, 3, 1]  \n",
    "\n",
    "sample_input = torch.tensor([[1., 0., -1.]])\n",
    "\n",
    "torch.manual_seed(123)\n",
    "model_without_shortcut = ExampleDeepNeuralNetwork(\n",
    "    layer_sizes, use_shortcut=False\n",
    ")\n",
    "print_gradients(model_without_shortcut, sample_input)\n",
    "\n",
    "\n",
    "torch.manual_seed(123)\n",
    "model_with_shortcut = ExampleDeepNeuralNetwork(\n",
    "layer_sizes, use_shortcut=True\n",
    ")\n",
    "print_gradients(model_with_shortcut, sample_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "cc52b65f-31df-4937-983d-6c5d2c5f9991",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.att = MultiHeadAttention(\n",
    "            d_in=cfg[\"emb_dim\"],\n",
    "            d_out=cfg[\"emb_dim\"],\n",
    "            context_length=cfg[\"context_length\"],\n",
    "            num_heads=cfg[\"n_heads\"], \n",
    "            dropout=cfg[\"drop_rate\"],\n",
    "            qkv_bias=cfg[\"qkv_bias\"])\n",
    "        self.ff = FeedForward(cfg)\n",
    "        self.norm1 = LayerNorm(cfg[\"emb_dim\"])\n",
    "        self.norm2 = LayerNorm(cfg[\"emb_dim\"])\n",
    "        self.drop_shortcut = nn.Dropout(cfg[\"drop_rate\"])\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Shortcut connection for attention block\n",
    "        shortcut = x\n",
    "        x = self.norm1(x)\n",
    "        x = self.att(x)  # Shape [batch_size, num_tokens, emb_size]\n",
    "        x = self.drop_shortcut(x)\n",
    "        x = x + shortcut  # Add the original input back\n",
    "\n",
    "        # Shortcut connection for feed forward block\n",
    "        shortcut = x\n",
    "        x = self.norm2(x)\n",
    "        x = self.ff(x)\n",
    "        x = self.drop_shortcut(x)\n",
    "        x = x + shortcut  # Add the original input back\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "b40cd825-2ec0-49d8-8d49-9527a16ea089",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([2, 4, 768])\n",
      "Output shape: torch.Size([2, 4, 768])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "x = torch.rand(2, 4, 768) #A\n",
    "block = TransformerBlock(GPT_CONFIG_124M)\n",
    "output = block(x)\n",
    "print(\"Input shape:\", x.shape)\n",
    "print(\"Output shape:\", output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "762d5ac5-3c83-409c-8452-7865e56fed75",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPTModel(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.tok_emb = nn.Embedding(cfg[\"vocab_size\"], cfg[\"emb_dim\"])\n",
    "        self.pos_emb = nn.Embedding(cfg[\"context_length\"], cfg[\"emb_dim\"])\n",
    "        self.drop_emb = nn.Dropout(cfg[\"drop_rate\"])\n",
    "        \n",
    "        self.trf_blocks = nn.Sequential(\n",
    "            *[TransformerBlock(cfg) for _ in range(cfg[\"n_layers\"])])\n",
    "        \n",
    "        self.final_norm = LayerNorm(cfg[\"emb_dim\"])\n",
    "        self.out_head = nn.Linear(\n",
    "            cfg[\"emb_dim\"], cfg[\"vocab_size\"], bias=False\n",
    "        )\n",
    "\n",
    "    def forward(self, in_idx):\n",
    "        batch_size, seq_len = in_idx.shape\n",
    "        tok_embeds = self.tok_emb(in_idx)\n",
    "        pos_embeds = self.pos_emb(torch.arange(seq_len, device=in_idx.device))\n",
    "        x = tok_embeds + pos_embeds  # Shape [batch_size, num_tokens, emb_size]\n",
    "        x = self.drop_emb(x)\n",
    "        x = self.trf_blocks(x)\n",
    "        x = self.final_norm(x)\n",
    "        logits = self.out_head(x)\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "df8e6e28-104e-4a4b-9821-330fb18e7f01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input batch:\n",
      " tensor([[6109, 3626, 6100,  345],\n",
      "        [6109, 1110, 6622,  257]])\n",
      "\n",
      "Output shape: torch.Size([2, 4, 50257])\n",
      "tensor([[[ 0.3613,  0.4222, -0.0711,  ...,  0.3483,  0.4661, -0.2838],\n",
      "         [-0.1792, -0.5660, -0.9485,  ...,  0.0477,  0.5181, -0.3168],\n",
      "         [ 0.7120,  0.0332,  0.1085,  ...,  0.1018, -0.4327, -0.2553],\n",
      "         [-1.0076,  0.3418, -0.1190,  ...,  0.7195,  0.4023,  0.0532]],\n",
      "\n",
      "        [[-0.2564,  0.0900,  0.0335,  ...,  0.2659,  0.4454, -0.6806],\n",
      "         [ 0.1230,  0.3653, -0.2074,  ...,  0.7705,  0.2710,  0.2246],\n",
      "         [ 1.0558,  1.0318, -0.2800,  ...,  0.6936,  0.3205, -0.3178],\n",
      "         [-0.1565,  0.3926,  0.3288,  ...,  1.2630, -0.1858,  0.0388]]],\n",
      "       grad_fn=<UnsafeViewBackward0>)\n",
      "\n",
      "Total number of parameters: 163,009,536\n",
      "Token embedding layer shape: torch.Size([50257, 768])\n",
      "Output layer shape: torch.Size([50257, 768])\n",
      "Number of trainable parameters considering weight tying: 124,412,160\n",
      "Total size of the model: 621.83 MB\n"
     ]
    }
   ],
   "source": [
    "# instantiate with random initial weights\n",
    "torch.manual_seed(123)\n",
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "\n",
    "out = model(batch)\n",
    "print(\"Input batch:\\n\", batch)\n",
    "print(\"\\nOutput shape:\", out.shape)\n",
    "print(out)\n",
    "\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"\\nTotal number of parameters: {total_params:,}\")\n",
    "\n",
    "print(\"Token embedding layer shape:\", model.tok_emb.weight.shape)\n",
    "print(\"Output layer shape:\", model.out_head.weight.shape)\n",
    "\n",
    "total_params_gpt2 = total_params - sum(p.numel() for p in model.out_head.parameters())\n",
    "print(f\"Number of trainable parameters considering weight tying: {total_params_gpt2:,}\")\n",
    "\n",
    "total_size_bytes = total_params * 4 #A 4 bytes\n",
    "total_size_mb = total_size_bytes / (1024 * 1024) #B\n",
    "print(f\"Total size of the model: {total_size_mb:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "de363a3d-7807-4f02-8e79-0c7add84fa08",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text_simple(model, idx, max_new_tokens, context_size):\n",
    "    # idx is (batch, n_tokens) array of indices in the current context\n",
    "    for _ in range(max_new_tokens):\n",
    "        \n",
    "        # Crop current context if it exceeds the supported context size\n",
    "        # E.g., if LLM supports only 5 tokens, and the context size is 10\n",
    "        # then only the last 5 tokens are used as context\n",
    "        idx_cond = idx[:, -context_size:]\n",
    "        \n",
    "        # Get the predictions\n",
    "        with torch.no_grad():\n",
    "            logits = model(idx_cond)\n",
    "        \n",
    "        # Focus only on the last time step\n",
    "        # (batch, n_tokens, vocab_size) becomes (batch, vocab_size)\n",
    "        logits = logits[:, -1, :]  \n",
    "\n",
    "        # Apply softmax to get probabilities\n",
    "        probas = torch.softmax(logits, dim=-1)  # (batch, vocab_size)\n",
    "\n",
    "        # Get the idx of the vocab entry with the highest probability value\n",
    "        idx_next = torch.argmax(probas, dim=-1, keepdim=True)  # (batch, 1)\n",
    "\n",
    "        # Append sampled index to the running sequence\n",
    "        idx = torch.cat((idx, idx_next), dim=1)  # (batch, n_tokens+1)\n",
    "\n",
    "    return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "c6d8857d-bb5a-4543-8c86-6ed1b17c7dd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoded: [15496, 11, 314, 716]\n",
      "encoded_tensor.shape: torch.Size([1, 4])\n"
     ]
    }
   ],
   "source": [
    "start_context = \"Hello, I am\"\n",
    "\n",
    "encoded = tokenizer.encode(start_context)\n",
    "print(\"encoded:\", encoded)\n",
    "\n",
    "encoded_tensor = torch.tensor(encoded).unsqueeze(0)\n",
    "print(\"encoded_tensor.shape:\", encoded_tensor.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "ab449ac7-5554-4ba2-8c8a-812af8f44a27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output: tensor([[15496,    11,   314,   716, 27018, 24086, 47843, 30961, 42348,  7267,\n",
      "         49706, 43231, 47062, 34657, 18631, 49188, 43312, 45933, 23154, 15983,\n",
      "         10345, 16369, 46214, 22954]])\n",
      "Output length: 24\n"
     ]
    }
   ],
   "source": [
    "model.eval() # disable dropout\n",
    "\n",
    "out = generate_text_simple(\n",
    "    model=model,\n",
    "    idx=encoded_tensor, \n",
    "    max_new_tokens=20, \n",
    "    context_size=GPT_CONFIG_124M[\"context_length\"]\n",
    ")\n",
    "\n",
    "print(\"Output:\", out)\n",
    "print(\"Output length:\", len(out[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "7a533549-5a8c-4965-a4ad-10f8281437d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, I am Featureiman Byeswickattribute argue logger Normandy Compton analogous bore ITVEGIN ministriesysics Kle functional recountrictionchanging\n"
     ]
    }
   ],
   "source": [
    "# generate text for the first time\n",
    "decoded_text = tokenizer.decode(out.squeeze(0).tolist())\n",
    "print(decoded_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "5555425d-ee1f-4840-a942-73ad0c87b929",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPTModel(\n",
       "  (tok_emb): Embedding(50257, 768)\n",
       "  (pos_emb): Embedding(256, 768)\n",
       "  (drop_emb): Dropout(p=0.1, inplace=False)\n",
       "  (trf_blocks): Sequential(\n",
       "    (0): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (1): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (2): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (3): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (4): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (5): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (6): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (7): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (8): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (9): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (10): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (11): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (final_norm): LayerNorm()\n",
       "  (out_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# chap 5\n",
    "\n",
    "GPT_CONFIG_124M = {\n",
    "    \"vocab_size\": 50257,\n",
    "    \"context_length\": 256, #A\n",
    "    \"emb_dim\": 768,\n",
    "    \"n_heads\": 12,\n",
    "    \"n_layers\": 12,\n",
    "    \"drop_rate\": 0.1, #B\n",
    "    \"qkv_bias\": False\n",
    "}\n",
    "torch.manual_seed(123)\n",
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "413d7190-7e29-42d1-a3a7-ab26552b3527",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output text:\n",
      " Every effort moves you rentingetic wasnم refres RexMeCHicular stren\n"
     ]
    }
   ],
   "source": [
    "def text_to_token_ids(text, tokenizer):\n",
    "    encoded = tokenizer.encode(text, allowed_special={'<|endoftext|>'})\n",
    "    encoded_tensor = torch.tensor(encoded).unsqueeze(0) # add batch dimension\n",
    "    return encoded_tensor\n",
    "\n",
    "def token_ids_to_text(token_ids, tokenizer):\n",
    "    flat = token_ids.squeeze(0) # remove batch dimension\n",
    "    return tokenizer.decode(flat.tolist())\n",
    "\n",
    "start_context = \"Every effort moves you\"\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "token_ids = generate_text_simple(\n",
    "    model=model,\n",
    "    idx=text_to_token_ids(start_context, tokenizer),\n",
    "    max_new_tokens=10,\n",
    "    context_size=GPT_CONFIG_124M[\"context_length\"]\n",
    ")\n",
    "\n",
    "print(\"Output text:\\n\", token_ids_to_text(token_ids, tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "fe305514-767d-41b0-a383-35bad7aec8b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3, 50257])\n",
      "Token IDs:\n",
      " tensor([[[16657],\n",
      "         [  339],\n",
      "         [42826]],\n",
      "\n",
      "        [[49906],\n",
      "         [29669],\n",
      "         [41751]]])\n",
      "Targets batch 1:  effort moves you\n",
      "Outputs batch 1:  Armed heNetflix\n",
      "Text 1: tensor([    0.0001,     0.0000,     0.0000])\n",
      "Text 2: tensor([    0.0000,     0.0001,     0.0000])\n"
     ]
    }
   ],
   "source": [
    "inputs = torch.tensor([[16833, 3626, 6100],   # [\"every effort moves\",   # using the double sliding window we can\n",
    "                       # know what tokens we actually are expecting to generate so we can gauge the quality of the generations\n",
    "                       [40,    1107, 588]])   #  \"I really like\"]\n",
    "\n",
    "targets = torch.tensor([[3626, 6100, 345  ],  # [\" effort moves you\",\n",
    "                        [1107,  588, 11311]]) #  \" really like chocolate\"]\n",
    "\n",
    "with torch.no_grad():\n",
    "    logits = model(inputs)\n",
    "\n",
    "probas = torch.softmax(logits, dim=-1) # Probability of each token in vocabulary\n",
    "print(probas.shape) # Shape: (batch_size, num_tokens, vocab_size)\n",
    "token_ids = torch.argmax(probas, dim=-1, keepdim=True)\n",
    "print(\"Token IDs:\\n\", token_ids)\n",
    "# starting to figure out how to validate quality of generations\n",
    "print(f\"Targets batch 1: {token_ids_to_text(targets[0], tokenizer)}\")\n",
    "print(f\"Outputs batch 1: {token_ids_to_text(token_ids[0].flatten(), tokenizer)}\")\n",
    "# we want to find out how far away from the correct answer we are\n",
    "#torch.set_printoptions(sci_mode=False)\n",
    "text_idx = 0\n",
    "target_probas_1 = probas[text_idx, [0, 1, 2], targets[text_idx]]\n",
    "print(\"Text 1:\", target_probas_1)\n",
    "\n",
    "text_idx = 1\n",
    "target_probas_2 = probas[text_idx, [0, 1, 2], targets[text_idx]]\n",
    "print(\"Text 2:\", target_probas_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "6e107d5b-ec65-4def-bd41-20045b3b07dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ -9.5042, -10.3796, -11.3677, -11.4798,  -9.7764, -12.2561])\n"
     ]
    }
   ],
   "source": [
    "# Compute logarithm of all token probabilities\n",
    "log_probas = torch.log(torch.cat((target_probas_1, target_probas_2)))\n",
    "print(log_probas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "2f01fd58-8ada-4e6c-8e95-34b2f3721f71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-10.7940)\n"
     ]
    }
   ],
   "source": [
    "# Calculate the average probability for each token\n",
    "avg_log_probas = torch.mean(log_probas)\n",
    "print(avg_log_probas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "9cb2626c-1ff0-4712-9010-2d0a730b64e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(10.7940)\n"
     ]
    }
   ],
   "source": [
    "neg_avg_log_probas = avg_log_probas * -1\n",
    "print(neg_avg_log_probas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "72908dd2-5699-4b83-bde0-3a8a2a93d052",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logits shape: torch.Size([2, 3, 50257])\n",
      "Targets shape: torch.Size([2, 3])\n"
     ]
    }
   ],
   "source": [
    "# Logits have shape (batch_size, num_tokens, vocab_size)\n",
    "print(\"Logits shape:\", logits.shape)\n",
    "\n",
    "# Targets have shape (batch_size, num_tokens)\n",
    "print(\"Targets shape:\", targets.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "34d4c5c6-a64e-4060-91dc-06d6611d0b02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Flattened logits: torch.Size([6, 50257])\n",
      "Flattened targets: torch.Size([6])\n"
     ]
    }
   ],
   "source": [
    "logits_flat = logits.flatten(0, 1)\n",
    "targets_flat = targets.flatten()\n",
    "\n",
    "print(\"Flattened logits:\", logits_flat.shape)\n",
    "print(\"Flattened targets:\", targets_flat.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "e2d8933f-600e-4e97-af48-82657d28da47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(10.7940)\n"
     ]
    }
   ],
   "source": [
    "loss = torch.nn.functional.cross_entropy(logits_flat, targets_flat)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "5d5e46c6-8257-45d1-b067-9863964375bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(48725.8203)\n"
     ]
    }
   ],
   "source": [
    "perplexity = torch.exp(loss)\n",
    "print(perplexity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "6bde3b61-6d9e-48c8-9fd5-8ae050fd6067",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Characters: 102463\n",
      "Tokens: 26663\n"
     ]
    }
   ],
   "source": [
    "total_characters = len(raw_text)\n",
    "total_tokens = len(tokenizer.encode(raw_text))\n",
    "\n",
    "print(\"Characters:\", total_characters)\n",
    "print(\"Tokens:\", total_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "4406031b-8fd6-4983-b3ae-be1e7b478475",
   "metadata": {},
   "outputs": [],
   "source": [
    "# start actual training\n",
    "\n",
    "# Train/validation ratio\n",
    "train_ratio = 0.90\n",
    "split_idx = int(train_ratio * len(raw_text))\n",
    "train_data = raw_text[:split_idx]\n",
    "val_data = raw_text[split_idx:]\n",
    "\n",
    "\n",
    "torch.manual_seed(123)\n",
    "\n",
    "train_loader = create_dataloader_v1(\n",
    "    train_data,\n",
    "    batch_size=2,\n",
    "    max_length=GPT_CONFIG_124M[\"context_length\"],\n",
    "    stride=GPT_CONFIG_124M[\"context_length\"],\n",
    "    drop_last=True,\n",
    "    shuffle=True,\n",
    "    #num_workers=0\n",
    ")\n",
    "\n",
    "val_loader = create_dataloader_v1(\n",
    "    val_data,\n",
    "    batch_size=2,\n",
    "    max_length=GPT_CONFIG_124M[\"context_length\"],\n",
    "    stride=GPT_CONFIG_124M[\"context_length\"],\n",
    "    drop_last=False,\n",
    "    shuffle=False,\n",
    "    #num_workers=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "0527a8ae-d87b-4af4-884d-e0db67fc74b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loader:\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "\n",
      "Validation loader:\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([1, 256]) torch.Size([1, 256])\n",
      "Training tokens: 24064\n",
      "Validation tokens: 2304\n",
      "All tokens: 26368\n"
     ]
    }
   ],
   "source": [
    "# validations on loaders\n",
    "print(\"Train loader:\")\n",
    "for x, y in train_loader:\n",
    "    print(x.shape, y.shape)\n",
    "print(\"\\nValidation loader:\")\n",
    "for x, y in val_loader:\n",
    "    print(x.shape, y.shape)\n",
    "\n",
    "if total_tokens * (train_ratio) < GPT_CONFIG_124M[\"context_length\"]:\n",
    "    print(\"Not enough tokens for the training loader. \"\n",
    "          \"Try to lower the `GPT_CONFIG_124M['context_length']` or \"\n",
    "          \"increase the `training_ratio`\")\n",
    "\n",
    "if total_tokens * (1-train_ratio) < GPT_CONFIG_124M[\"context_length\"]:\n",
    "    print(\"Not enough tokens for the validation loader. \"\n",
    "          \"Try to lower the `GPT_CONFIG_124M['context_length']` or \"\n",
    "          \"decrease the `training_ratio`\")\n",
    "\n",
    "train_tokens = 0\n",
    "for input_batch, target_batch in train_loader:\n",
    "    train_tokens += input_batch.numel()\n",
    "\n",
    "val_tokens = 0\n",
    "for input_batch, target_batch in val_loader:\n",
    "    val_tokens += input_batch.numel()\n",
    "\n",
    "print(\"Training tokens:\", train_tokens)\n",
    "print(\"Validation tokens:\", val_tokens)\n",
    "print(\"All tokens:\", train_tokens + val_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "288eadd6-9b5c-4f14-b7a2-e44ab4a7df2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_loss_batch(input_batch, target_batch, model, device):\n",
    "    input_batch, target_batch = input_batch.to(device), target_batch.to(device)\n",
    "    logits = model(input_batch)\n",
    "    loss = torch.nn.functional.cross_entropy(logits.flatten(0, 1), target_batch.flatten())\n",
    "    return loss\n",
    "\n",
    "\n",
    "def calc_loss_loader(data_loader, model, device, num_batches=None):\n",
    "    total_loss = 0.\n",
    "    if len(data_loader) == 0:\n",
    "        return float(\"nan\")\n",
    "    elif num_batches is None:\n",
    "        num_batches = len(data_loader)\n",
    "    else:\n",
    "        # Reduce the number of batches to match the total number of batches in the data loader\n",
    "        # if num_batches exceeds the number of batches in the data loader\n",
    "        num_batches = min(num_batches, len(data_loader))\n",
    "    for i, (input_batch, target_batch) in enumerate(data_loader):\n",
    "        if i < num_batches:\n",
    "            loss = calc_loss_batch(input_batch, target_batch, model, device)\n",
    "            total_loss += loss.item()\n",
    "        else:\n",
    "            break\n",
    "    return total_loss / num_batches\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "b1465620-899d-4951-8bc6-5704a60b4192",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "Training loss: 10.997001546494504\n",
      "Validation loss: 10.974541664123535\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "print(device)\n",
    "model.to(device) # no assignment model = model.to(device) necessary for nn.Module classes\n",
    "\n",
    "\n",
    "torch.manual_seed(123) # For reproducibility due to the shuffling in the data loader\n",
    "\n",
    "with torch.no_grad(): # Disable gradient tracking for efficiency because we are not training, yet\n",
    "    train_loss = calc_loss_loader(train_loader, model, device)\n",
    "    val_loss = calc_loss_loader(val_loader, model, device)\n",
    "\n",
    "print(\"Training loss:\", train_loss)\n",
    "print(\"Validation loss:\", val_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "5cf98d78-bc07-4b19-ab8c-48f72c943f36",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_simple(model, train_loader, val_loader, optimizer, device, num_epochs,\n",
    "                       eval_freq, eval_iter, start_context, tokenizer):\n",
    "    # Initialize lists to track losses and tokens seen\n",
    "    train_losses, val_losses, track_tokens_seen = [], [], []\n",
    "    tokens_seen, global_step = 0, -1\n",
    "\n",
    "    # Main training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()  # Set model to training mode\n",
    "        \n",
    "        for input_batch, target_batch in train_loader:\n",
    "            optimizer.zero_grad() # Reset loss gradients from previous batch iteration\n",
    "            loss = calc_loss_batch(input_batch, target_batch, model, device)\n",
    "            loss.backward() # Calculate loss gradients\n",
    "            optimizer.step() # Update model weights using loss gradients\n",
    "            tokens_seen += input_batch.numel()\n",
    "            global_step += 1\n",
    "\n",
    "            # Optional evaluation step\n",
    "            if global_step % eval_freq == 0:\n",
    "                train_loss, val_loss = evaluate_model(\n",
    "                    model, train_loader, val_loader, device, eval_iter)\n",
    "                train_losses.append(train_loss)\n",
    "                val_losses.append(val_loss)\n",
    "                track_tokens_seen.append(tokens_seen)\n",
    "                print(f\"Ep {epoch+1} (Step {global_step:06d}): \"\n",
    "                      f\"Train loss {train_loss:.3f}, Val loss {val_loss:.3f}\")\n",
    "\n",
    "        # Print a sample text after each epoch\n",
    "        generate_and_print_sample(\n",
    "            model, tokenizer, device, start_context\n",
    "        )\n",
    "\n",
    "    return train_losses, val_losses, track_tokens_seen\n",
    "\n",
    "\n",
    "def evaluate_model(model, train_loader, val_loader, device, eval_iter):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        train_loss = calc_loss_loader(train_loader, model, device, num_batches=eval_iter)\n",
    "        val_loss = calc_loss_loader(val_loader, model, device, num_batches=eval_iter)\n",
    "    model.train()\n",
    "    return train_loss, val_loss\n",
    "\n",
    "\n",
    "def generate_and_print_sample(model, tokenizer, device, start_context):\n",
    "    model.eval()\n",
    "    context_size = model.pos_emb.weight.shape[0]\n",
    "    encoded = text_to_token_ids(start_context, tokenizer).to(device)\n",
    "    with torch.no_grad():\n",
    "        token_ids = generate_text_simple(\n",
    "            model=model, idx=encoded,\n",
    "            max_new_tokens=50, context_size=context_size\n",
    "        )\n",
    "    decoded_text = token_ids_to_text(token_ids, tokenizer)\n",
    "    print(decoded_text.replace(\"\\n\", \" \"))  # Compact print format\n",
    "    model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "94e7369e-bd1a-499e-9670-f3a7c6e87aeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 1 (Step 000000): Train loss 9.755, Val loss 9.842\n",
      "Ep 1 (Step 000005): Train loss 8.157, Val loss 8.706\n",
      "Ep 1 (Step 000010): Train loss 7.373, Val loss 7.965\n",
      "Ep 1 (Step 000015): Train loss 6.988, Val loss 7.908\n",
      "Ep 1 (Step 000020): Train loss 6.352, Val loss 7.915\n",
      "Ep 1 (Step 000025): Train loss 6.121, Val loss 7.736\n",
      "Ep 1 (Step 000030): Train loss 6.275, Val loss 7.591\n",
      "Ep 1 (Step 000035): Train loss 6.304, Val loss 7.465\n",
      "Ep 1 (Step 000040): Train loss 6.321, Val loss 7.431\n",
      "Ep 1 (Step 000045): Train loss 5.785, Val loss 7.446\n",
      "Every effort moves you the                                                 \n",
      "Ep 2 (Step 000050): Train loss 6.117, Val loss 7.497\n",
      "Ep 2 (Step 000055): Train loss 5.778, Val loss 7.512\n",
      "Ep 2 (Step 000060): Train loss 5.917, Val loss 7.590\n",
      "Ep 2 (Step 000065): Train loss 5.980, Val loss 7.600\n",
      "Ep 2 (Step 000070): Train loss 5.767, Val loss 7.603\n",
      "Ep 2 (Step 000075): Train loss 5.874, Val loss 7.449\n",
      "Ep 2 (Step 000080): Train loss 5.766, Val loss 7.492\n",
      "Ep 2 (Step 000085): Train loss 5.594, Val loss 7.539\n",
      "Ep 2 (Step 000090): Train loss 5.424, Val loss 7.604\n",
      "Every effort moves you, and the            The the  The the, and the, and the    The The The the  The, and the  The   The\n",
      "Ep 3 (Step 000095): Train loss 5.236, Val loss 7.642\n",
      "Ep 3 (Step 000100): Train loss 5.590, Val loss 7.460\n",
      "Ep 3 (Step 000105): Train loss 5.417, Val loss 7.494\n",
      "Ep 3 (Step 000110): Train loss 5.048, Val loss 7.641\n",
      "Ep 3 (Step 000115): Train loss 5.269, Val loss 7.465\n",
      "Ep 3 (Step 000120): Train loss 5.378, Val loss 7.407\n",
      "Ep 3 (Step 000125): Train loss 4.942, Val loss 7.390\n",
      "Ep 3 (Step 000130): Train loss 5.195, Val loss 7.450\n",
      "Ep 3 (Step 000135): Train loss 4.513, Val loss 7.356\n",
      "Ep 3 (Step 000140): Train loss 4.409, Val loss 7.273\n",
      "Every effort moves you.                                                 \n",
      "Ep 4 (Step 000145): Train loss 4.469, Val loss 7.228\n",
      "Ep 4 (Step 000150): Train loss 4.372, Val loss 7.120\n",
      "Ep 4 (Step 000155): Train loss 4.600, Val loss 7.056\n",
      "Ep 4 (Step 000160): Train loss 4.423, Val loss 7.101\n",
      "Ep 4 (Step 000165): Train loss 4.203, Val loss 7.284\n",
      "Ep 4 (Step 000170): Train loss 4.341, Val loss 7.339\n",
      "Ep 4 (Step 000175): Train loss 3.968, Val loss 7.313\n",
      "Ep 4 (Step 000180): Train loss 4.001, Val loss 7.186\n",
      "Ep 4 (Step 000185): Train loss 4.186, Val loss 7.287\n",
      "Every effort moves you in the The girls, and the The girls were driven to the girls were driven to be the girls were driven to the The girls of the The girls.    The girls were driven to the girls were very The\n",
      "Ep 5 (Step 000190): Train loss 3.441, Val loss 7.222\n",
      "Ep 5 (Step 000195): Train loss 3.650, Val loss 7.169\n",
      "Ep 5 (Step 000200): Train loss 3.581, Val loss 7.172\n",
      "Ep 5 (Step 000205): Train loss 3.558, Val loss 7.109\n",
      "Ep 5 (Step 000210): Train loss 3.331, Val loss 7.216\n",
      "Ep 5 (Step 000215): Train loss 3.068, Val loss 7.288\n",
      "Ep 5 (Step 000220): Train loss 3.052, Val loss 7.251\n",
      "Ep 5 (Step 000225): Train loss 3.242, Val loss 7.352\n",
      "Ep 5 (Step 000230): Train loss 3.137, Val loss 7.368\n",
      "Every effort moves you will  and the    and the  The first and was the  and the The girls, and the   The girls were all the and is a good of the   and is the\n",
      "Ep 6 (Step 000235): Train loss 3.123, Val loss 7.401\n",
      "Ep 6 (Step 000240): Train loss 2.870, Val loss 7.396\n",
      "Ep 6 (Step 000245): Train loss 2.633, Val loss 7.307\n",
      "Ep 6 (Step 000250): Train loss 2.837, Val loss 7.370\n",
      "Ep 6 (Step 000255): Train loss 2.373, Val loss 7.293\n",
      "Ep 6 (Step 000260): Train loss 2.623, Val loss 7.340\n",
      "Ep 6 (Step 000265): Train loss 2.479, Val loss 7.351\n",
      "Ep 6 (Step 000270): Train loss 2.455, Val loss 7.292\n",
      "Ep 6 (Step 000275): Train loss 2.051, Val loss 7.300\n",
      "Ep 6 (Step 000280): Train loss 2.135, Val loss 7.291\n",
      "Every effort moves you will  \"Oh, and said Mr.      \"Oh, \"I think it was produced from images  \"Oh, and \"Oh, and \"Oh, and Mr.     \n",
      "Ep 7 (Step 000285): Train loss 2.087, Val loss 7.377\n",
      "Ep 7 (Step 000290): Train loss 2.121, Val loss 7.302\n",
      "Ep 7 (Step 000295): Train loss 2.178, Val loss 7.355\n",
      "Ep 7 (Step 000300): Train loss 1.792, Val loss 7.329\n",
      "Ep 7 (Step 000305): Train loss 1.739, Val loss 7.353\n",
      "Ep 7 (Step 000310): Train loss 1.539, Val loss 7.362\n",
      "Ep 7 (Step 000315): Train loss 1.808, Val loss 7.470\n",
      "Ep 7 (Step 000320): Train loss 1.633, Val loss 7.541\n",
      "Ep 7 (Step 000325): Train loss 1.495, Val loss 7.465\n",
      "Every effort moves you are located before we have to the city to the soldiers being of the first part of the The girls of the water,  The cathedral was found very  The trip was very delightful rooms overlooking the river.     The\n",
      "Ep 8 (Step 000330): Train loss 1.517, Val loss 7.529\n",
      "Ep 8 (Step 000335): Train loss 1.189, Val loss 7.540\n",
      "Ep 8 (Step 000340): Train loss 1.272, Val loss 7.672\n",
      "Ep 8 (Step 000345): Train loss 1.264, Val loss 7.536\n",
      "Ep 8 (Step 000350): Train loss 1.342, Val loss 7.519\n",
      "Ep 8 (Step 000355): Train loss 0.942, Val loss 7.606\n",
      "Ep 8 (Step 000360): Train loss 1.157, Val loss 7.608\n",
      "Ep 8 (Step 000365): Train loss 0.978, Val loss 7.553\n",
      "Ep 8 (Step 000370): Train loss 0.950, Val loss 7.625\n",
      "Ep 8 (Step 000375): Train loss 1.000, Val loss 7.630\n",
      "Every effort moves you can.\"      \"That is the car window, dear,\" said her papa, and you?\"  \"Oh, yes, I do not much less than our fences and many such objects of     \n",
      "Ep 9 (Step 000380): Train loss 1.025, Val loss 7.670\n",
      "Ep 9 (Step 000385): Train loss 0.807, Val loss 7.685\n",
      "Ep 9 (Step 000390): Train loss 0.810, Val loss 7.702\n",
      "Ep 9 (Step 000395): Train loss 0.853, Val loss 7.753\n",
      "Ep 9 (Step 000400): Train loss 0.775, Val loss 7.781\n",
      "Ep 9 (Step 000405): Train loss 0.656, Val loss 7.740\n",
      "Ep 9 (Step 000410): Train loss 0.627, Val loss 7.698\n",
      "Ep 9 (Step 000415): Train loss 0.590, Val loss 7.844\n",
      "Ep 9 (Step 000420): Train loss 0.687, Val loss 7.955\n",
      "Every effort moves you as  [Illustration: ALBRECHT DÜRER'S HOUSE.--_Page 74._]  The cathedral was founded in Paris, as all these one to our  at home!\" said, \"I\n",
      "Ep 10 (Step 000425): Train loss 0.562, Val loss 7.836\n",
      "Ep 10 (Step 000430): Train loss 0.553, Val loss 7.883\n",
      "Ep 10 (Step 000435): Train loss 0.677, Val loss 7.831\n",
      "Ep 10 (Step 000440): Train loss 0.482, Val loss 7.919\n",
      "Ep 10 (Step 000445): Train loss 0.395, Val loss 7.858\n",
      "Ep 10 (Step 000450): Train loss 0.443, Val loss 7.815\n",
      "Ep 10 (Step 000455): Train loss 0.480, Val loss 7.907\n",
      "Ep 10 (Step 000460): Train loss 0.533, Val loss 8.108\n",
      "Ep 10 (Step 000465): Train loss 0.496, Val loss 8.178\n",
      "Every effort moves you can and after looking at the scenery they walked on up to the castle.  [Illustration: NUREMBERG CASTLE.]  On the way they saw a small shed, and, on looking in, found it held\n",
      "Ep 11 (Step 000470): Train loss 0.441, Val loss 7.865\n",
      "Ep 11 (Step 000475): Train loss 0.422, Val loss 7.995\n",
      "Ep 11 (Step 000480): Train loss 0.340, Val loss 8.034\n",
      "Ep 11 (Step 000485): Train loss 0.432, Val loss 8.025\n",
      "Ep 11 (Step 000490): Train loss 0.490, Val loss 8.019\n",
      "Ep 11 (Step 000495): Train loss 0.500, Val loss 8.071\n",
      "Ep 11 (Step 000500): Train loss 0.389, Val loss 7.962\n",
      "Ep 11 (Step 000505): Train loss 0.408, Val loss 8.064\n",
      "Ep 11 (Step 000510): Train loss 0.360, Val loss 8.010\n",
      "Ep 11 (Step 000515): Train loss 0.325, Val loss 7.863\n",
      "Every effort moves you can go home, may sometime give as much pleasure to another as you have given me.\"  Mr. Winter said, \"Why, Agnes, I never heard you recite that poem so well.\"   Wednesday at eleven o\n",
      "Ep 12 (Step 000520): Train loss 0.293, Val loss 7.937\n",
      "Ep 12 (Step 000525): Train loss 0.310, Val loss 8.054\n",
      "Ep 12 (Step 000530): Train loss 0.364, Val loss 8.206\n",
      "Ep 12 (Step 000535): Train loss 0.242, Val loss 8.083\n",
      "Ep 12 (Step 000540): Train loss 0.420, Val loss 8.182\n",
      "Ep 12 (Step 000545): Train loss 0.277, Val loss 8.173\n",
      "Ep 12 (Step 000550): Train loss 0.328, Val loss 8.211\n",
      "Ep 12 (Step 000555): Train loss 0.224, Val loss 8.216\n",
      "Ep 12 (Step 000560): Train loss 0.206, Val loss 8.189\n",
      "Every effort moves you can   \"Oh, yes,\" said Mr. Ford, \"I think it must be the next station.\"   The statue of spirits.   The girls were delighted, \"I have it was dark when Hôtel\n",
      "Ep 13 (Step 000565): Train loss 0.288, Val loss 8.167\n",
      "Ep 13 (Step 000570): Train loss 0.213, Val loss 8.255\n",
      "Ep 13 (Step 000575): Train loss 0.318, Val loss 8.241\n",
      "Ep 13 (Step 000580): Train loss 0.192, Val loss 8.058\n",
      "Ep 13 (Step 000585): Train loss 0.267, Val loss 8.149\n",
      "Ep 13 (Step 000590): Train loss 0.220, Val loss 8.144\n",
      "Ep 13 (Step 000595): Train loss 0.271, Val loss 8.300\n",
      "Ep 13 (Step 000600): Train loss 0.193, Val loss 8.360\n",
      "Ep 13 (Step 000605): Train loss 0.137, Val loss 8.280\n",
      "Ep 13 (Step 000610): Train loss 0.141, Val loss 8.142\n",
      "Every effort moves you sick  After leaving the cathedral our party was driven around the city. To their surprise they found at the  Whispering Gallery. Both went down the harbor until the passengers, among them our own. Mrs. Winter said Alice. \n",
      "Ep 14 (Step 000615): Train loss 0.147, Val loss 8.240\n",
      "Ep 14 (Step 000620): Train loss 0.215, Val loss 8.329\n",
      "Ep 14 (Step 000625): Train loss 0.190, Val loss 8.388\n",
      "Ep 14 (Step 000630): Train loss 0.145, Val loss 8.363\n",
      "Ep 14 (Step 000635): Train loss 0.106, Val loss 8.398\n",
      "Ep 14 (Step 000640): Train loss 0.177, Val loss 8.358\n",
      "Ep 14 (Step 000645): Train loss 0.155, Val loss 8.391\n",
      "Ep 14 (Step 000650): Train loss 0.267, Val loss 8.284\n",
      "Ep 14 (Step 000655): Train loss 0.155, Val loss 8.220\n",
      "Every effort moves you    \"Oh, yes,\" said Mr. Winter called a The steamer started immediately, \"and is now being used to notify the people who are not sailing on the steamer that it is time to go ashore\n",
      "Ep 15 (Step 000660): Train loss 0.390, Val loss 8.397\n",
      "Ep 15 (Step 000665): Train loss 0.294, Val loss 8.519\n",
      "Ep 15 (Step 000670): Train loss 0.119, Val loss 8.482\n",
      "Ep 15 (Step 000675): Train loss 0.176, Val loss 8.361\n",
      "Ep 15 (Step 000680): Train loss 0.106, Val loss 8.411\n",
      "Ep 15 (Step 000685): Train loss 0.186, Val loss 8.378\n",
      "Ep 15 (Step 000690): Train loss 0.103, Val loss 8.541\n",
      "Ep 15 (Step 000695): Train loss 0.107, Val loss 8.412\n",
      "Ep 15 (Step 000700): Train loss 0.075, Val loss 8.646\n",
      "Every effort moves you will the terms of your time going around with a guide or by yourselves.\"  Alice was delighted with the old moat which was opposite her window, and wanted to look in it at once.  Nellie felt the\n",
      "Ep 16 (Step 000705): Train loss 0.158, Val loss 8.821\n",
      "Ep 16 (Step 000710): Train loss 0.121, Val loss 8.713\n",
      "Ep 16 (Step 000715): Train loss 0.130, Val loss 8.521\n",
      "Ep 16 (Step 000720): Train loss 0.120, Val loss 8.514\n",
      "Ep 16 (Step 000725): Train loss 0.156, Val loss 8.546\n",
      "Ep 16 (Step 000730): Train loss 0.158, Val loss 8.601\n",
      "Ep 16 (Step 000735): Train loss 0.122, Val loss 8.561\n",
      "Ep 16 (Step 000740): Train loss 0.061, Val loss 8.442\n",
      "Ep 16 (Step 000745): Train loss 0.087, Val loss 8.575\n",
      "Ep 16 (Step 000750): Train loss 0.053, Val loss 8.490\n",
      "Every effort moves you can never been used in any way, but stands as a warning to despotic people.\"  Mr. Winter said, \"Alice, you told that very well; but he was not such a very wicked man as the legend makes him\n",
      "Ep 17 (Step 000755): Train loss 0.128, Val loss 8.524\n",
      "Ep 17 (Step 000760): Train loss 0.107, Val loss 8.710\n",
      "Ep 17 (Step 000765): Train loss 0.109, Val loss 8.727\n",
      "Ep 17 (Step 000770): Train loss 0.063, Val loss 8.738\n",
      "Ep 17 (Step 000775): Train loss 0.084, Val loss 8.686\n",
      "Ep 17 (Step 000780): Train loss 0.075, Val loss 8.667\n",
      "Ep 17 (Step 000785): Train loss 0.032, Val loss 8.432\n",
      "Ep 17 (Step 000790): Train loss 0.169, Val loss 8.517\n",
      "Ep 17 (Step 000795): Train loss 0.091, Val loss 8.628\n",
      "Every effort moves you can.\"  \"Oh, yes,\" said Mr. Ford, \"I think it must be the next station.\"  When they reached it, it was not theirs, and Mr. Ford called out to the guard, \"How many\n",
      "Ep 18 (Step 000800): Train loss 0.062, Val loss 8.651\n",
      "Ep 18 (Step 000805): Train loss 0.033, Val loss 8.569\n",
      "Ep 18 (Step 000810): Train loss 0.036, Val loss 8.581\n",
      "Ep 18 (Step 000815): Train loss 0.059, Val loss 8.607\n",
      "Ep 18 (Step 000820): Train loss 0.188, Val loss 8.612\n",
      "Ep 18 (Step 000825): Train loss 0.118, Val loss 8.672\n",
      "Ep 18 (Step 000830): Train loss 0.054, Val loss 8.858\n",
      "Ep 18 (Step 000835): Train loss 0.020, Val loss 8.517\n",
      "Ep 18 (Step 000840): Train loss 0.030, Val loss 8.507\n",
      "Ep 18 (Step 000845): Train loss 0.034, Val loss 8.587\n",
      "Every effort moves you can.\"  Alice, what do you know about this?\" Mr. Winter's neck, crying with joy, saying at the same time, \"Oh, you dear, darling papa, how kind and good you are, and\n",
      "Ep 19 (Step 000850): Train loss 0.037, Val loss 8.831\n",
      "Ep 19 (Step 000855): Train loss 0.136, Val loss 8.794\n",
      "Ep 19 (Step 000860): Train loss 0.078, Val loss 8.608\n",
      "Ep 19 (Step 000865): Train loss 0.080, Val loss 8.592\n",
      "Ep 19 (Step 000870): Train loss 0.116, Val loss 8.661\n",
      "Ep 19 (Step 000875): Train loss 0.093, Val loss 8.638\n",
      "Ep 19 (Step 000880): Train loss 0.020, Val loss 8.718\n",
      "Ep 19 (Step 000885): Train loss 0.070, Val loss 8.750\n",
      "Ep 19 (Step 000890): Train loss 0.139, Val loss 8.678\n",
      "Every effort moves you there?\" Mr. Winter said to his wife, \"I have really finished my business this side of the water, and unless you would like to remain in London three or four more days for the 'Etruria,' we can catch\n",
      "Ep 20 (Step 000895): Train loss 0.069, Val loss 8.654\n",
      "Ep 20 (Step 000900): Train loss 0.074, Val loss 8.560\n",
      "Ep 20 (Step 000905): Train loss 0.110, Val loss 8.752\n",
      "Ep 20 (Step 000910): Train loss 0.122, Val loss 8.710\n",
      "Ep 20 (Step 000915): Train loss 0.171, Val loss 8.866\n",
      "Ep 20 (Step 000920): Train loss 0.293, Val loss 8.606\n",
      "Ep 20 (Step 000925): Train loss 0.315, Val loss 8.833\n",
      "Ep 20 (Step 000930): Train loss 0.302, Val loss 8.466\n",
      "Ep 20 (Step 000935): Train loss 0.258, Val loss 8.357\n",
      "Every effort moves you will to dinner avarice of the laws corridors and given away—you may be seen again, crying with joy._]  valley furnishings.all the United States with an individual work is unprotected by copyright law in their minds. Hatto ordered\n",
      "Training completed in 3.26 minutes.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "torch.manual_seed(123)\n",
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "model.to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.0004, weight_decay=0.1)\n",
    "\n",
    "num_epochs = 20\n",
    "train_losses, val_losses, tokens_seen = train_model_simple(\n",
    "    model, train_loader, val_loader, optimizer, device,\n",
    "    num_epochs=num_epochs, eval_freq=5, eval_iter=5,\n",
    "    start_context=\"Every effort moves you\", tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "\n",
    "end_time = time.time()\n",
    "execution_time_minutes = (end_time - start_time) / 60\n",
    "print(f\"Training completed in {execution_time_minutes:.2f} minutes.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "e48e9ba8-c919-4f2d-8d8c-611e978b1263",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x300 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "\n",
    "\n",
    "def plot_losses(epochs_seen, tokens_seen, train_losses, val_losses):\n",
    "    fig, ax1 = plt.subplots(figsize=(5, 3))\n",
    "\n",
    "    # Plot training and validation loss against epochs\n",
    "    ax1.plot(epochs_seen, train_losses, label=\"Training loss\")\n",
    "    ax1.plot(epochs_seen, val_losses, linestyle=\"-.\", label=\"Validation loss\")\n",
    "    ax1.set_xlabel(\"Epochs\")\n",
    "    ax1.set_ylabel(\"Loss\")\n",
    "    ax1.legend(loc=\"upper right\")\n",
    "    ax1.xaxis.set_major_locator(MaxNLocator(integer=True))  # only show integer labels on x-axis\n",
    "\n",
    "    # Create a second x-axis for tokens seen\n",
    "    ax2 = ax1.twiny()  # Create a second x-axis that shares the same y-axis\n",
    "    ax2.plot(tokens_seen, train_losses, alpha=0)  # Invisible plot for aligning ticks\n",
    "    ax2.set_xlabel(\"Tokens seen\")\n",
    "\n",
    "    fig.tight_layout()  # Adjust layout to make room\n",
    "    plt.savefig(\"loss-plot.pdf\")\n",
    "    plt.show()\n",
    "\n",
    "epochs_tensor = torch.linspace(0, num_epochs, len(train_losses))\n",
    "plot_losses(epochs_tensor, tokens_seen, train_losses, val_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "ff08a8a0-94d3-44f3-a765-a096b1ef9bbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output text:\n",
      " Every effort moves you will to dinner avarice of the laws corridors and given away—you may be seen again, crying with joy._]\n"
     ]
    }
   ],
   "source": [
    "model.to(\"cpu\")\n",
    "model.eval()\n",
    "\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "token_ids = generate_text_simple(\n",
    "    model=model,\n",
    "    idx=text_to_token_ids(\"Every effort moves you\", tokenizer),\n",
    "    max_new_tokens=25,\n",
    "    context_size=GPT_CONFIG_124M[\"context_length\"]\n",
    ")\n",
    "\n",
    "print(\"Output text:\\n\", token_ids_to_text(token_ids, tokenizer))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "e4d1ccef-7224-4cbf-95e1-cc7618b70d07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "forward\n"
     ]
    }
   ],
   "source": [
    "vocab = { \n",
    "    \"closer\": 0,\n",
    "    \"every\": 1, \n",
    "    \"effort\": 2, \n",
    "    \"forward\": 3,\n",
    "    \"inches\": 4,\n",
    "    \"moves\": 5, \n",
    "    \"pizza\": 6,\n",
    "    \"toward\": 7,\n",
    "    \"you\": 8,\n",
    "} \n",
    "\n",
    "inverse_vocab = {v: k for k, v in vocab.items()}\n",
    "\n",
    "# Suppose input is \"every effort moves you\", and the LLM\n",
    "# returns the following logits for the next token:\n",
    "next_token_logits = torch.tensor(\n",
    "    [4.51, 0.89, -1.90, 6.75, 1.63, -1.62, -1.89, 6.28, 1.79]\n",
    ")\n",
    "\n",
    "probas = torch.softmax(next_token_logits, dim=0)\n",
    "next_token_id = torch.argmax(probas).item()\n",
    "\n",
    "# The next generated token is then as follows:\n",
    "print(inverse_vocab[next_token_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "549d21cf-aca2-47c3-9071-17b0ef92cc19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "forward\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "next_token_id = torch.multinomial(probas, num_samples=1).item()\n",
    "print(inverse_vocab[next_token_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "2dbee204-3cbc-4854-b65c-3b567270ebef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "73 x closer\n",
      "0 x every\n",
      "0 x effort\n",
      "582 x forward\n",
      "2 x inches\n",
      "0 x moves\n",
      "0 x pizza\n",
      "343 x toward\n"
     ]
    }
   ],
   "source": [
    "def print_sampled_tokens(probas):\n",
    "    torch.manual_seed(123) # Manual seed for reproducibility\n",
    "    sample = [torch.multinomial(probas, num_samples=1).item() for i in range(1_000)]\n",
    "    sampled_ids = torch.bincount(torch.tensor(sample))\n",
    "    for i, freq in enumerate(sampled_ids):\n",
    "        print(f\"{freq} x {inverse_vocab[i]}\")\n",
    "\n",
    "print_sampled_tokens(probas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "e9b0f796-6146-4929-9325-804b61e1fd74",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax_with_temperature(logits, temperature):\n",
    "    scaled_logits = logits / temperature\n",
    "    return torch.softmax(scaled_logits, dim=0)\n",
    "\n",
    "# Temperature values\n",
    "temperatures = [1, 0.1, 5]  # Original, higher confidence, and lower confidence\n",
    "\n",
    "# Calculate scaled probabilities\n",
    "scaled_probas = [softmax_with_temperature(next_token_logits, T) for T in temperatures]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "ec3e8525-5a60-46e3-a3e1-c1bddaf36d3d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x300 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plotting\n",
    "x = torch.arange(len(vocab))\n",
    "bar_width = 0.15\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(5, 3))\n",
    "for i, T in enumerate(temperatures):\n",
    "    rects = ax.bar(x + i * bar_width, scaled_probas[i], bar_width, label=f'Temperature = {T}')\n",
    "\n",
    "ax.set_ylabel('Probability')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(vocab.keys(), rotation=90)\n",
    "ax.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"temperature-plot.pdf\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "5da4f163-fa71-4b4b-b97f-7d1ae579aa2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 x closer\n",
      "0 x every\n",
      "0 x effort\n",
      "985 x forward\n",
      "0 x inches\n",
      "0 x moves\n",
      "0 x pizza\n",
      "15 x toward\n"
     ]
    }
   ],
   "source": [
    "print_sampled_tokens(scaled_probas[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "57c51ddd-6b68-4417-9598-b790b5a93a29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "165 x closer\n",
      "75 x every\n",
      "42 x effort\n",
      "239 x forward\n",
      "71 x inches\n",
      "46 x moves\n",
      "32 x pizza\n",
      "227 x toward\n",
      "103 x you\n"
     ]
    }
   ],
   "source": [
    "print_sampled_tokens(scaled_probas[2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "bf368a12-6371-4cc9-92a4-0fd0904ac12b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top logits: tensor([6.7500, 6.2800, 4.5100])\n",
      "Top positions: tensor([3, 7, 0])\n"
     ]
    }
   ],
   "source": [
    "top_k = 3\n",
    "top_logits, top_pos = torch.topk(next_token_logits, top_k)\n",
    "\n",
    "print(\"Top logits:\", top_logits)\n",
    "print(\"Top positions:\", top_pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "7e226e85-a3c0-44eb-a89e-90181751ea4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([4.5100,   -inf,   -inf, 6.7500,   -inf,   -inf,   -inf, 6.2800,   -inf])\n"
     ]
    }
   ],
   "source": [
    "new_logits = torch.where(\n",
    "    condition=next_token_logits < top_logits[-1],\n",
    "    input=torch.tensor(float(\"-inf\")), \n",
    "    other=next_token_logits\n",
    ")\n",
    "\n",
    "print(new_logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "88af3b14-8291-4c0e-9a0c-e4887b03a1ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.0615, 0.0000, 0.0000, 0.5775, 0.0000, 0.0000, 0.0000, 0.3610, 0.0000])\n"
     ]
    }
   ],
   "source": [
    "topk_probas = torch.softmax(new_logits, dim=0)\n",
    "print(topk_probas)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "e88fcdd8-1192-4c29-9e6a-3a03fb930432",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(model, idx, max_new_tokens, context_size, temperature=0.0, top_k=None, eos_id=None):\n",
    "\n",
    "    # For-loop is the same as before: Get logits, and only focus on last time step\n",
    "    for _ in range(max_new_tokens):\n",
    "        idx_cond = idx[:, -context_size:]\n",
    "        with torch.no_grad():\n",
    "            logits = model(idx_cond)\n",
    "        logits = logits[:, -1, :]\n",
    "\n",
    "        # New: Filter logits with top_k sampling\n",
    "        if top_k is not None:\n",
    "            # Keep only top_k values\n",
    "            top_logits, _ = torch.topk(logits, top_k)\n",
    "            min_val = top_logits[:, -1]\n",
    "            logits = torch.where(logits < min_val, torch.tensor(float(\"-inf\")).to(logits.device), logits)\n",
    "\n",
    "        # New: Apply temperature scaling\n",
    "        if temperature > 0.0:\n",
    "            logits = logits / temperature\n",
    "\n",
    "            # Apply softmax to get probabilities\n",
    "            probs = torch.softmax(logits, dim=-1)  # (batch_size, context_len)\n",
    "\n",
    "            # Sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)  # (batch_size, 1)\n",
    "\n",
    "        # Otherwise same as before: get idx of the vocab entry with the highest logits value\n",
    "        else:\n",
    "            idx_next = torch.argmax(logits, dim=-1, keepdim=True)  # (batch_size, 1)\n",
    "\n",
    "        if idx_next == eos_id:  # Stop generating early if end-of-sequence token is encountered and eos_id is specified\n",
    "            break\n",
    "\n",
    "        # Same as before: append sampled index to the running sequence\n",
    "        idx = torch.cat((idx, idx_next), dim=1)  # (batch_size, num_tokens+1)\n",
    "\n",
    "    return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "208128bd-ede2-4f6c-bb34-62bc8144230a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output text:\n",
      " Every effort moves you will to each full make a fog there weighs four hundred using and used country\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "\n",
    "token_ids = generate(\n",
    "    model=model,\n",
    "    idx=text_to_token_ids(\"Every effort moves you\", tokenizer),\n",
    "    max_new_tokens=15,\n",
    "    context_size=GPT_CONFIG_124M[\"context_length\"],\n",
    "    top_k=25,\n",
    "    temperature=1.4\n",
    ")\n",
    "\n",
    "print(\"Output text:\\n\", token_ids_to_text(token_ids, tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "c61996bc-341a-4e7e-ba85-c22a1477b91e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving model weights\n",
    "#torch.save(model.state_dict(), \"model.pth\") # basic saving\n",
    "\n",
    "torch.save({\n",
    "    \"model_state_dict\": model.state_dict(),\n",
    "    \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "    }, \n",
    "    \"model_and_optimizer.pth\"\n",
    ") # saving with additional parameters, for future finetuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "db7fbd56-cebf-46fb-996d-e15eaab736e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here's how to load a model\n",
    "#model = GPTModel(GPT_CONFIG_124M)\n",
    "#device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "#model.load_state_dict(torch.load(\"model_and_optimizer.pth\", map_location=device, weights_only=True))\n",
    "#model.eval();\n",
    "\n",
    "# how to load model for future finetuning\n",
    "\n",
    "#checkpoint = torch.load(\"model_and_optimizer.pth\", weights_only=True)\n",
    "\n",
    "#model = GPTModel(GPT_CONFIG_124M)\n",
    "#model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "\n",
    "#optimizer = torch.optim.AdamW(model.parameters(), lr=0.0005, weight_decay=0.1)\n",
    "#optimizer.load_state_dict(checkpoint[\"optimizer_state_dict\"])\n",
    "#model.train();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "e5e8a1ba-7c73-4f7d-9d8e-fe4812d7fcbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numpy version: 2.2.3\n",
      "torch version: 2.8.0.dev20250324+rocm6.3\n",
      "transformers version: 4.49.0\n"
     ]
    }
   ],
   "source": [
    "# loading some weights from the original openai weights\n",
    "# needs: poetry add transformers \n",
    "# going off the alternate loading\n",
    "\n",
    "from importlib.metadata import version\n",
    "\n",
    "pkgs = [\"numpy\", \"torch\", \"transformers\"]\n",
    "for p in pkgs:\n",
    "    print(f\"{p} version: {version(p)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "7646e965-8d52-4544-bc39-fb6060f22a33",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f7176a89e1043159c194f5923a99777",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "533b8eccc7a34ddc9b0c5a09165d14db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/548M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "GPT2Model(\n",
       "  (wte): Embedding(50257, 768)\n",
       "  (wpe): Embedding(1024, 768)\n",
       "  (drop): Dropout(p=0.1, inplace=False)\n",
       "  (h): ModuleList(\n",
       "    (0-11): 12 x GPT2Block(\n",
       "      (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (attn): GPT2Attention(\n",
       "        (c_attn): Conv1D(nf=2304, nx=768)\n",
       "        (c_proj): Conv1D(nf=768, nx=768)\n",
       "        (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "        (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp): GPT2MLP(\n",
       "        (c_fc): Conv1D(nf=3072, nx=768)\n",
       "        (c_proj): Conv1D(nf=768, nx=3072)\n",
       "        (act): NewGELUActivation()\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       ")"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import GPT2Model\n",
    "\n",
    "\n",
    "# allowed model names\n",
    "model_names = {\n",
    "    \"gpt2-small (124M)\": \"openai-community/gpt2\",\n",
    "    \"gpt2-medium (355M)\": \"openai-community/gpt2-medium\",\n",
    "    \"gpt2-large (774M)\": \"openai-community/gpt2-large\",\n",
    "    \"gpt2-xl (1558M)\": \"openai-community/gpt2-xl\"\n",
    "}\n",
    "\n",
    "CHOOSE_MODEL = \"gpt2-small (124M)\"\n",
    "\n",
    "gpt_hf = GPT2Model.from_pretrained(model_names[CHOOSE_MODEL], cache_dir=\"checkpoints\")\n",
    "gpt_hf.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "a76e7015-6313-45c1-9092-fe49bf42ea0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_CONFIG = {\n",
    "    \"vocab_size\": 50257,    # Vocabulary size\n",
    "    \"context_length\": 1024, # Context length\n",
    "    \"drop_rate\": 0.0,       # Dropout rate\n",
    "    \"qkv_bias\": True        # Query-key-value bias\n",
    "}\n",
    "\n",
    "model_configs = {\n",
    "    \"gpt2-small (124M)\": {\"emb_dim\": 768, \"n_layers\": 12, \"n_heads\": 12},\n",
    "    \"gpt2-medium (355M)\": {\"emb_dim\": 1024, \"n_layers\": 24, \"n_heads\": 16},\n",
    "    \"gpt2-large (774M)\": {\"emb_dim\": 1280, \"n_layers\": 36, \"n_heads\": 20},\n",
    "    \"gpt2-xl (1558M)\": {\"emb_dim\": 1600, \"n_layers\": 48, \"n_heads\": 25},\n",
    "}\n",
    "\n",
    "\n",
    "BASE_CONFIG.update(model_configs[CHOOSE_MODEL])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "6bd1b9c9-bb3a-4e32-b65e-5ec456c93487",
   "metadata": {},
   "outputs": [],
   "source": [
    "def assign_check(left, right):\n",
    "    if left.shape != right.shape:\n",
    "        raise ValueError(f\"Shape mismatch. Left: {left.shape}, Right: {right.shape}\")\n",
    "    return torch.nn.Parameter(right.clone().detach())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "ce2b5517-4bb1-4c28-966c-d82212edd63c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def load_weights(gpt, gpt_hf):\n",
    "\n",
    "    d = gpt_hf.state_dict()\n",
    "\n",
    "    gpt.pos_emb.weight = assign_check(gpt.pos_emb.weight, d[\"wpe.weight\"])\n",
    "    gpt.tok_emb.weight = assign_check(gpt.tok_emb.weight, d[\"wte.weight\"])\n",
    "    \n",
    "    for b in range(BASE_CONFIG[\"n_layers\"]):\n",
    "        q_w, k_w, v_w = np.split(d[f\"h.{b}.attn.c_attn.weight\"], 3, axis=-1)\n",
    "        gpt.trf_blocks[b].att.W_query.weight = assign_check(gpt.trf_blocks[b].att.W_query.weight, q_w.T)\n",
    "        gpt.trf_blocks[b].att.W_key.weight = assign_check(gpt.trf_blocks[b].att.W_key.weight, k_w.T)\n",
    "        gpt.trf_blocks[b].att.W_value.weight = assign_check(gpt.trf_blocks[b].att.W_value.weight, v_w.T)\n",
    "    \n",
    "        q_b, k_b, v_b = np.split(d[f\"h.{b}.attn.c_attn.bias\"], 3, axis=-1)\n",
    "        gpt.trf_blocks[b].att.W_query.bias = assign_check(gpt.trf_blocks[b].att.W_query.bias, q_b)\n",
    "        gpt.trf_blocks[b].att.W_key.bias = assign_check(gpt.trf_blocks[b].att.W_key.bias, k_b)\n",
    "        gpt.trf_blocks[b].att.W_value.bias = assign_check(gpt.trf_blocks[b].att.W_value.bias, v_b)\n",
    "    \n",
    "    \n",
    "        gpt.trf_blocks[b].att.out_proj.weight = assign_check(gpt.trf_blocks[b].att.out_proj.weight, d[f\"h.{b}.attn.c_proj.weight\"].T)\n",
    "        gpt.trf_blocks[b].att.out_proj.bias = assign_check(gpt.trf_blocks[b].att.out_proj.bias, d[f\"h.{b}.attn.c_proj.bias\"])\n",
    "    \n",
    "        gpt.trf_blocks[b].ff.layers[0].weight = assign_check(gpt.trf_blocks[b].ff.layers[0].weight, d[f\"h.{b}.mlp.c_fc.weight\"].T)\n",
    "        gpt.trf_blocks[b].ff.layers[0].bias = assign_check(gpt.trf_blocks[b].ff.layers[0].bias, d[f\"h.{b}.mlp.c_fc.bias\"])\n",
    "        gpt.trf_blocks[b].ff.layers[2].weight = assign_check(gpt.trf_blocks[b].ff.layers[2].weight, d[f\"h.{b}.mlp.c_proj.weight\"].T)\n",
    "        gpt.trf_blocks[b].ff.layers[2].bias = assign_check(gpt.trf_blocks[b].ff.layers[2].bias, d[f\"h.{b}.mlp.c_proj.bias\"])\n",
    "    \n",
    "        gpt.trf_blocks[b].norm1.scale = assign_check(gpt.trf_blocks[b].norm1.scale, d[f\"h.{b}.ln_1.weight\"])\n",
    "        gpt.trf_blocks[b].norm1.shift = assign_check(gpt.trf_blocks[b].norm1.shift, d[f\"h.{b}.ln_1.bias\"])\n",
    "        gpt.trf_blocks[b].norm2.scale = assign_check(gpt.trf_blocks[b].norm2.scale, d[f\"h.{b}.ln_2.weight\"])\n",
    "        gpt.trf_blocks[b].norm2.shift = assign_check(gpt.trf_blocks[b].norm2.shift, d[f\"h.{b}.ln_2.bias\"])\n",
    "    \n",
    "        gpt.final_norm.scale = assign_check(gpt.final_norm.scale, d[\"ln_f.weight\"])\n",
    "        gpt.final_norm.shift = assign_check(gpt.final_norm.shift, d[\"ln_f.bias\"])\n",
    "        gpt.out_head.weight = assign_check(gpt.out_head.weight, d[\"wte.weight\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "15b19f7a-8a5a-4b7b-aca1-48e49f094a97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "gpt = GPTModel(BASE_CONFIG)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "load_weights(gpt, gpt_hf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "1128b8c2-2de0-44b2-80b8-ed3cb45c9987",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output text:\n",
      " Every effort moves forward, but it's not enough.\n",
      "\n",
      "\"I'm not going to sit here and say, 'I'm not going to do this,'\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "\n",
    "#tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "token_ids = generate(\n",
    "    model=gpt.to(device),\n",
    "    idx=text_to_token_ids(\"Every effort moves\", tokenizer).to(device),\n",
    "    max_new_tokens=30,\n",
    "    context_size=BASE_CONFIG[\"context_length\"],\n",
    "    top_k=1,\n",
    "    temperature=1.0\n",
    ")\n",
    "\n",
    "print(\"Output text:\\n\", token_ids_to_text(token_ids, tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "1f68443d-680b-4bef-88a5-9d059edb61bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of entries: 1100\n",
      "Example entry:\n",
      " {'instruction': 'Identify the correct spelling of the following word.', 'input': 'Ocassion', 'output': \"The correct spelling is 'Occasion.'\"}\n"
     ]
    }
   ],
   "source": [
    "# onto fine-tuning\n",
    "import json\n",
    "import os\n",
    "import urllib\n",
    "\n",
    "\n",
    "def download_and_load_file(file_path, url):\n",
    "\n",
    "    if not os.path.exists(file_path):\n",
    "        with urllib.request.urlopen(url) as response:\n",
    "            text_data = response.read().decode(\"utf-8\")\n",
    "        with open(file_path, \"w\", encoding=\"utf-8\") as file:\n",
    "            file.write(text_data)\n",
    "\n",
    "\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "        data = json.load(file)\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "file_path = \"instruction-data.json\"\n",
    "url = (\n",
    "    \"https://raw.githubusercontent.com/rasbt/LLMs-from-scratch\"\n",
    "    \"/main/ch07/01_main-chapter-code/instruction-data.json\"\n",
    ")\n",
    "\n",
    "data = download_and_load_file(file_path, url)\n",
    "print(\"Number of entries:\", len(data))\n",
    "\n",
    "\n",
    "print(\"Example entry:\\n\", data[50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "ea53eef3-bbdd-450e-804f-273e24bfa1a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_input(entry):\n",
    "    instruction_text = (\n",
    "        f\"Below is an instruction that describes a task. \"\n",
    "        f\"Write a response that appropriately completes the request.\"\n",
    "        f\"\\n\\n### Instruction:\\n{entry['instruction']}\"\n",
    "    )\n",
    "\n",
    "    input_text = f\"\\n\\n### Input:\\n{entry['input']}\" if entry[\"input\"] else \"\"\n",
    "\n",
    "    return instruction_text + input_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "c69dffe9-0737-4870-9b00-35b054bf834d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Identify the correct spelling of the following word.\n",
      "\n",
      "### Input:\n",
      "Ocassion\n",
      "\n",
      "### Response:\n",
      "The correct spelling is 'Occasion.'\n"
     ]
    }
   ],
   "source": [
    "model_input = format_input(data[50])\n",
    "desired_response = f\"\\n\\n### Response:\\n{data[50]['output']}\"\n",
    "\n",
    "print(model_input + desired_response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "92e28ee6-84b6-41d7-8c83-73d8852bc05d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "What is an antonym of 'complicated'?\n",
      "\n",
      "### Response:\n",
      "An antonym of 'complicated' is 'simple'.\n"
     ]
    }
   ],
   "source": [
    "model_input = format_input(data[999])\n",
    "desired_response = f\"\\n\\n### Response:\\n{data[999]['output']}\"\n",
    "\n",
    "print(model_input + desired_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "58248716-54e3-4ffd-932c-45b26c7347e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_portion = int(len(data) * 0.85)  # 85% for training\n",
    "test_portion = int(len(data) * 0.1)    # 10% for testing\n",
    "val_portion = len(data) - train_portion - test_portion  # Remaining 5% for validation\n",
    "\n",
    "train_data = data[:train_portion]\n",
    "test_data = data[train_portion:train_portion + test_portion]\n",
    "val_data = data[train_portion + test_portion:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "a927f99a-70df-4227-9f69-62746a76872c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set length: 935\n",
      "Validation set length: 55\n",
      "Test set length: 110\n"
     ]
    }
   ],
   "source": [
    "print(\"Training set length:\", len(train_data))\n",
    "print(\"Validation set length:\", len(val_data))\n",
    "print(\"Test set length:\", len(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "c8f6b898-e8f4-43db-8cc6-447fd8d5c3b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "\n",
    "class InstructionDataset(Dataset):\n",
    "    def __init__(self, data, tokenizer):\n",
    "        self.data = data\n",
    "\n",
    "        # Pre-tokenize texts\n",
    "        self.encoded_texts = []\n",
    "        for entry in data:\n",
    "            instruction_plus_input = format_input(entry)\n",
    "            response_text = f\"\\n\\n### Response:\\n{entry['output']}\"\n",
    "            full_text = instruction_plus_input + response_text\n",
    "            self.encoded_texts.append(\n",
    "                tokenizer.encode(full_text)\n",
    "            )\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.encoded_texts[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "f685d376-eaee-45c2-aaa8-6bd198a19745",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[50256]\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.encode(\"<|endoftext|>\", allowed_special={\"<|endoftext|>\"}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "6088f21a-3c96-4f0d-a7a4-717eac1e4dcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_collate_draft_1(\n",
    "    batch,\n",
    "    pad_token_id=50256,\n",
    "    device=\"cpu\"\n",
    "):\n",
    "    # Find the longest sequence in the batch\n",
    "    # and increase the max length by +1, which will add one extra\n",
    "    # padding token below\n",
    "    batch_max_length = max(len(item)+1 for item in batch)\n",
    "\n",
    "    # Pad and prepare inputs\n",
    "    inputs_lst = []\n",
    "\n",
    "    for item in batch:\n",
    "        new_item = item.copy()\n",
    "        # Add an <|endoftext|> token\n",
    "        new_item += [pad_token_id]\n",
    "        # Pad sequences to batch_max_length\n",
    "        padded = (\n",
    "            new_item + [pad_token_id] *\n",
    "            (batch_max_length - len(new_item))\n",
    "        )\n",
    "        # Via padded[:-1], we remove the extra padded token\n",
    "        # that has been added via the +1 setting in batch_max_length\n",
    "        # (the extra padding token will be relevant in later codes)\n",
    "        inputs = torch.tensor(padded[:-1])\n",
    "        inputs_lst.append(inputs)\n",
    "\n",
    "    # Convert list of inputs to tensor and transfer to target device\n",
    "    inputs_tensor = torch.stack(inputs_lst).to(device)\n",
    "    return inputs_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "88403884-f036-4475-a7a4-069fecd5c020",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[    0,     1,     2,     3,     4],\n",
      "        [    5,     6, 50256, 50256, 50256],\n",
      "        [    7,     8,     9, 50256, 50256]])\n"
     ]
    }
   ],
   "source": [
    "inputs_1 = [0, 1, 2, 3, 4]\n",
    "inputs_2 = [5, 6]\n",
    "inputs_3 = [7, 8, 9]\n",
    "\n",
    "batch = (\n",
    "    inputs_1,\n",
    "    inputs_2,\n",
    "    inputs_3\n",
    ")\n",
    "\n",
    "print(custom_collate_draft_1(batch))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "08bedb69-b62c-430b-87fd-4dfbc89ccaf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_collate_draft_2(\n",
    "    batch,\n",
    "    pad_token_id=50256,\n",
    "    device=\"cpu\"\n",
    "):\n",
    "    # Find the longest sequence in the batch\n",
    "    batch_max_length = max(len(item)+1 for item in batch)\n",
    "\n",
    "    # Pad and prepare inputs\n",
    "    inputs_lst, targets_lst = [], []\n",
    "\n",
    "    for item in batch:\n",
    "        new_item = item.copy()\n",
    "        # Add an <|endoftext|> token\n",
    "        new_item += [pad_token_id]\n",
    "        # Pad sequences to max_length\n",
    "        padded = (\n",
    "            new_item + [pad_token_id] *\n",
    "            (batch_max_length - len(new_item))\n",
    "        )\n",
    "        inputs = torch.tensor(padded[:-1])  # Truncate the last token for inputs\n",
    "        targets = torch.tensor(padded[1:])  # Shift +1 to the right for targets\n",
    "        inputs_lst.append(inputs)\n",
    "        targets_lst.append(targets)\n",
    "\n",
    "    # Convert list of inputs to tensor and transfer to target device\n",
    "    inputs_tensor = torch.stack(inputs_lst).to(device)\n",
    "    targets_tensor = torch.stack(targets_lst).to(device)\n",
    "    return inputs_tensor, targets_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "c038295f-55dd-49a6-babd-84abffce19c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[    0,     1,     2,     3,     4],\n",
      "        [    5,     6, 50256, 50256, 50256],\n",
      "        [    7,     8,     9, 50256, 50256]])\n",
      "tensor([[    1,     2,     3,     4, 50256],\n",
      "        [    6, 50256, 50256, 50256, 50256],\n",
      "        [    8,     9, 50256, 50256, 50256]])\n"
     ]
    }
   ],
   "source": [
    "inputs, targets = custom_collate_draft_2(batch)\n",
    "print(inputs)\n",
    "print(targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "febe3a57-ef55-4019-94a4-5db6012e8aea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_collate_fn(\n",
    "    batch,\n",
    "    pad_token_id=50256,\n",
    "    ignore_index=-100,\n",
    "    allowed_max_length=None,\n",
    "    device=\"cpu\"\n",
    "):\n",
    "    # Find the longest sequence in the batch\n",
    "    batch_max_length = max(len(item)+1 for item in batch)\n",
    "\n",
    "    # Pad and prepare inputs and targets\n",
    "    inputs_lst, targets_lst = [], []\n",
    "\n",
    "    for item in batch:\n",
    "        new_item = item.copy()\n",
    "        # Add an <|endoftext|> token\n",
    "        new_item += [pad_token_id]\n",
    "        # Pad sequences to max_length\n",
    "        padded = (\n",
    "            new_item + [pad_token_id] *\n",
    "            (batch_max_length - len(new_item))\n",
    "        )\n",
    "        inputs = torch.tensor(padded[:-1])  # Truncate the last token for inputs\n",
    "        targets = torch.tensor(padded[1:])  # Shift +1 to the right for targets\n",
    "\n",
    "        # New: Replace all but the first padding tokens in targets by ignore_index\n",
    "        mask = targets == pad_token_id\n",
    "        indices = torch.nonzero(mask).squeeze()\n",
    "        if indices.numel() > 1:\n",
    "            targets[indices[1:]] = ignore_index\n",
    "\n",
    "        # New: Optionally truncate to maximum sequence length\n",
    "        if allowed_max_length is not None:\n",
    "            inputs = inputs[:allowed_max_length]\n",
    "            targets = targets[:allowed_max_length]\n",
    "\n",
    "        inputs_lst.append(inputs)\n",
    "        targets_lst.append(targets)\n",
    "\n",
    "    # Convert list of inputs and targets to tensors and transfer to target device\n",
    "    inputs_tensor = torch.stack(inputs_lst).to(device)\n",
    "    targets_tensor = torch.stack(targets_lst).to(device)\n",
    "\n",
    "    return inputs_tensor, targets_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "8a90b68e-93e7-44fe-b9d3-aab6e5e34b40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[    0,     1,     2,     3,     4],\n",
      "        [    5,     6, 50256, 50256, 50256],\n",
      "        [    7,     8,     9, 50256, 50256]])\n",
      "tensor([[    1,     2,     3,     4, 50256],\n",
      "        [    6, 50256,  -100,  -100,  -100],\n",
      "        [    8,     9, 50256,  -100,  -100]])\n"
     ]
    }
   ],
   "source": [
    "inputs, targets = custom_collate_fn(batch)\n",
    "print(inputs)\n",
    "print(targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "d0956ce9-e08d-45ed-9df2-1c3dfd14199a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.1269)\n"
     ]
    }
   ],
   "source": [
    "logits_1 = torch.tensor(\n",
    "    [[-1.0, 1.0],  # 1st training example\n",
    "     [-0.5, 1.5]]  # 2nd training example\n",
    ")\n",
    "targets_1 = torch.tensor([0, 1])\n",
    "\n",
    "\n",
    "loss_1 = torch.nn.functional.cross_entropy(logits_1, targets_1)\n",
    "print(loss_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "a42c622e-a302-40bf-a378-6f4a37e80570",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.7936)\n"
     ]
    }
   ],
   "source": [
    "logits_2 = torch.tensor(\n",
    "    [[-1.0, 1.0],\n",
    "     [-0.5, 1.5],\n",
    "     [-0.5, 1.5]]  # New 3rd training example\n",
    ")\n",
    "targets_2 = torch.tensor([0, 1, 1])\n",
    "\n",
    "loss_2 = torch.nn.functional.cross_entropy(logits_2, targets_2)\n",
    "print(loss_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "08b1785d-0822-4337-8f69-d8a2a257053f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.1269)\n",
      "loss_1 == loss_3: tensor(True)\n"
     ]
    }
   ],
   "source": [
    "targets_3 = torch.tensor([0, 1, -100])\n",
    "\n",
    "loss_3 = torch.nn.functional.cross_entropy(logits_2, targets_3)\n",
    "print(loss_3)\n",
    "print(\"loss_1 == loss_3:\", loss_1 == loss_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "81b37673-9c7d-4014-a6f3-154a1fbe5589",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "e00e6434-d0bb-4640-b44e-f07533e22209",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "customized_collate_fn = partial(\n",
    "    custom_collate_fn,\n",
    "    device=device,\n",
    "    allowed_max_length=1024\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "d635e2a8-d22d-467c-887c-ecec57516bf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "num_workers = 0\n",
    "batch_size = 8\n",
    "\n",
    "torch.manual_seed(123)\n",
    "\n",
    "train_dataset = InstructionDataset(train_data, tokenizer)\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    collate_fn=customized_collate_fn,\n",
    "    shuffle=True,\n",
    "    drop_last=True,\n",
    "    num_workers=num_workers\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "7afd4b28-c075-4858-bb57-d99b1989d467",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataset = InstructionDataset(val_data, tokenizer)\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=batch_size,\n",
    "    collate_fn=customized_collate_fn,\n",
    "    shuffle=False,\n",
    "    drop_last=False,\n",
    "    num_workers=num_workers\n",
    ")\n",
    "\n",
    "test_dataset = InstructionDataset(test_data, tokenizer)\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=batch_size,\n",
    "    collate_fn=customized_collate_fn,\n",
    "    shuffle=False,\n",
    "    drop_last=False,\n",
    "    num_workers=num_workers\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "cfffcf46-cbf0-46c0-8c32-8855e2424fbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loader:\n",
      "torch.Size([8, 61]) torch.Size([8, 61])\n",
      "torch.Size([8, 76]) torch.Size([8, 76])\n",
      "torch.Size([8, 73]) torch.Size([8, 73])\n",
      "torch.Size([8, 68]) torch.Size([8, 68])\n",
      "torch.Size([8, 65]) torch.Size([8, 65])\n",
      "torch.Size([8, 72]) torch.Size([8, 72])\n",
      "torch.Size([8, 80]) torch.Size([8, 80])\n",
      "torch.Size([8, 67]) torch.Size([8, 67])\n",
      "torch.Size([8, 62]) torch.Size([8, 62])\n",
      "torch.Size([8, 75]) torch.Size([8, 75])\n",
      "torch.Size([8, 62]) torch.Size([8, 62])\n",
      "torch.Size([8, 68]) torch.Size([8, 68])\n",
      "torch.Size([8, 67]) torch.Size([8, 67])\n",
      "torch.Size([8, 77]) torch.Size([8, 77])\n",
      "torch.Size([8, 69]) torch.Size([8, 69])\n",
      "torch.Size([8, 79]) torch.Size([8, 79])\n",
      "torch.Size([8, 71]) torch.Size([8, 71])\n",
      "torch.Size([8, 66]) torch.Size([8, 66])\n",
      "torch.Size([8, 83]) torch.Size([8, 83])\n",
      "torch.Size([8, 68]) torch.Size([8, 68])\n",
      "torch.Size([8, 80]) torch.Size([8, 80])\n",
      "torch.Size([8, 71]) torch.Size([8, 71])\n",
      "torch.Size([8, 69]) torch.Size([8, 69])\n",
      "torch.Size([8, 65]) torch.Size([8, 65])\n",
      "torch.Size([8, 68]) torch.Size([8, 68])\n",
      "torch.Size([8, 60]) torch.Size([8, 60])\n",
      "torch.Size([8, 59]) torch.Size([8, 59])\n",
      "torch.Size([8, 69]) torch.Size([8, 69])\n",
      "torch.Size([8, 63]) torch.Size([8, 63])\n",
      "torch.Size([8, 65]) torch.Size([8, 65])\n",
      "torch.Size([8, 76]) torch.Size([8, 76])\n",
      "torch.Size([8, 66]) torch.Size([8, 66])\n",
      "torch.Size([8, 71]) torch.Size([8, 71])\n",
      "torch.Size([8, 91]) torch.Size([8, 91])\n",
      "torch.Size([8, 65]) torch.Size([8, 65])\n",
      "torch.Size([8, 64]) torch.Size([8, 64])\n",
      "torch.Size([8, 67]) torch.Size([8, 67])\n",
      "torch.Size([8, 66]) torch.Size([8, 66])\n",
      "torch.Size([8, 64]) torch.Size([8, 64])\n",
      "torch.Size([8, 65]) torch.Size([8, 65])\n",
      "torch.Size([8, 75]) torch.Size([8, 75])\n",
      "torch.Size([8, 89]) torch.Size([8, 89])\n",
      "torch.Size([8, 59]) torch.Size([8, 59])\n",
      "torch.Size([8, 88]) torch.Size([8, 88])\n",
      "torch.Size([8, 83]) torch.Size([8, 83])\n",
      "torch.Size([8, 83]) torch.Size([8, 83])\n",
      "torch.Size([8, 70]) torch.Size([8, 70])\n",
      "torch.Size([8, 65]) torch.Size([8, 65])\n",
      "torch.Size([8, 74]) torch.Size([8, 74])\n",
      "torch.Size([8, 76]) torch.Size([8, 76])\n",
      "torch.Size([8, 67]) torch.Size([8, 67])\n",
      "torch.Size([8, 75]) torch.Size([8, 75])\n",
      "torch.Size([8, 83]) torch.Size([8, 83])\n",
      "torch.Size([8, 69]) torch.Size([8, 69])\n",
      "torch.Size([8, 67]) torch.Size([8, 67])\n",
      "torch.Size([8, 60]) torch.Size([8, 60])\n",
      "torch.Size([8, 60]) torch.Size([8, 60])\n",
      "torch.Size([8, 66]) torch.Size([8, 66])\n",
      "torch.Size([8, 80]) torch.Size([8, 80])\n",
      "torch.Size([8, 71]) torch.Size([8, 71])\n",
      "torch.Size([8, 61]) torch.Size([8, 61])\n",
      "torch.Size([8, 58]) torch.Size([8, 58])\n",
      "torch.Size([8, 71]) torch.Size([8, 71])\n",
      "torch.Size([8, 67]) torch.Size([8, 67])\n",
      "torch.Size([8, 68]) torch.Size([8, 68])\n",
      "torch.Size([8, 63]) torch.Size([8, 63])\n",
      "torch.Size([8, 87]) torch.Size([8, 87])\n",
      "torch.Size([8, 68]) torch.Size([8, 68])\n",
      "torch.Size([8, 64]) torch.Size([8, 64])\n",
      "torch.Size([8, 68]) torch.Size([8, 68])\n",
      "torch.Size([8, 71]) torch.Size([8, 71])\n",
      "torch.Size([8, 68]) torch.Size([8, 68])\n",
      "torch.Size([8, 71]) torch.Size([8, 71])\n",
      "torch.Size([8, 61]) torch.Size([8, 61])\n",
      "torch.Size([8, 65]) torch.Size([8, 65])\n",
      "torch.Size([8, 67]) torch.Size([8, 67])\n",
      "torch.Size([8, 65]) torch.Size([8, 65])\n",
      "torch.Size([8, 64]) torch.Size([8, 64])\n",
      "torch.Size([8, 60]) torch.Size([8, 60])\n",
      "torch.Size([8, 72]) torch.Size([8, 72])\n",
      "torch.Size([8, 64]) torch.Size([8, 64])\n",
      "torch.Size([8, 70]) torch.Size([8, 70])\n",
      "torch.Size([8, 57]) torch.Size([8, 57])\n",
      "torch.Size([8, 72]) torch.Size([8, 72])\n",
      "torch.Size([8, 64]) torch.Size([8, 64])\n",
      "torch.Size([8, 68]) torch.Size([8, 68])\n",
      "torch.Size([8, 62]) torch.Size([8, 62])\n",
      "torch.Size([8, 74]) torch.Size([8, 74])\n",
      "torch.Size([8, 80]) torch.Size([8, 80])\n",
      "torch.Size([8, 68]) torch.Size([8, 68])\n",
      "torch.Size([8, 70]) torch.Size([8, 70])\n",
      "torch.Size([8, 91]) torch.Size([8, 91])\n",
      "torch.Size([8, 61]) torch.Size([8, 61])\n",
      "torch.Size([8, 66]) torch.Size([8, 66])\n",
      "torch.Size([8, 80]) torch.Size([8, 80])\n",
      "torch.Size([8, 81]) torch.Size([8, 81])\n",
      "torch.Size([8, 74]) torch.Size([8, 74])\n",
      "torch.Size([8, 82]) torch.Size([8, 82])\n",
      "torch.Size([8, 63]) torch.Size([8, 63])\n",
      "torch.Size([8, 83]) torch.Size([8, 83])\n",
      "torch.Size([8, 68]) torch.Size([8, 68])\n",
      "torch.Size([8, 67]) torch.Size([8, 67])\n",
      "torch.Size([8, 77]) torch.Size([8, 77])\n",
      "torch.Size([8, 91]) torch.Size([8, 91])\n",
      "torch.Size([8, 64]) torch.Size([8, 64])\n",
      "torch.Size([8, 61]) torch.Size([8, 61])\n",
      "torch.Size([8, 75]) torch.Size([8, 75])\n",
      "torch.Size([8, 64]) torch.Size([8, 64])\n",
      "torch.Size([8, 66]) torch.Size([8, 66])\n",
      "torch.Size([8, 78]) torch.Size([8, 78])\n",
      "torch.Size([8, 66]) torch.Size([8, 66])\n",
      "torch.Size([8, 64]) torch.Size([8, 64])\n",
      "torch.Size([8, 83]) torch.Size([8, 83])\n",
      "torch.Size([8, 66]) torch.Size([8, 66])\n",
      "torch.Size([8, 74]) torch.Size([8, 74])\n",
      "torch.Size([8, 69]) torch.Size([8, 69])\n"
     ]
    }
   ],
   "source": [
    "print(\"Train loader:\")\n",
    "for inputs, targets in train_loader:\n",
    "    print(inputs.shape, targets.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "d68fd4c7-94e4-4da8-b59a-65cc2e99c4ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([21106,   318,   281, 12064,   326,  8477,   257,  4876,    13, 19430,\n",
      "          257,  2882,   326, 20431, 32543,   262,  2581,    13,   198,   198,\n",
      "        21017, 46486,    25,   198, 30003,  6525,   262,  6827,  1262,   257,\n",
      "          985,   576,    13,   198,   198, 21017, 23412,    25,   198,   464,\n",
      "         5156,   318,   845, 13779,    13,   198,   198, 21017, 18261,    25,\n",
      "          198,   464,  5156,   318,   355, 13779,   355,   257,  4936,    13,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256],\n",
      "       device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "print(inputs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "c4ad49ce-3d4b-4552-a106-bb91477e6102",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([  318,   281, 12064,   326,  8477,   257,  4876,    13, 19430,   257,\n",
      "         2882,   326, 20431, 32543,   262,  2581,    13,   198,   198, 21017,\n",
      "        46486,    25,   198, 30003,  6525,   262,  6827,  1262,   257,   985,\n",
      "          576,    13,   198,   198, 21017, 23412,    25,   198,   464,  5156,\n",
      "          318,   845, 13779,    13,   198,   198, 21017, 18261,    25,   198,\n",
      "          464,  5156,   318,   355, 13779,   355,   257,  4936,    13, 50256,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100],\n",
      "       device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "print(targets[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "11e37740-a50f-42c6-ac8c-f5ff1cc85c73",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f79a41623c504fba9b37f96e38d2baa1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/718 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d960ecebbec9443193f0e60015281628",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.52G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "GPT2Model(\n",
       "  (wte): Embedding(50257, 1024)\n",
       "  (wpe): Embedding(1024, 1024)\n",
       "  (drop): Dropout(p=0.1, inplace=False)\n",
       "  (h): ModuleList(\n",
       "    (0-23): 24 x GPT2Block(\n",
       "      (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      (attn): GPT2Attention(\n",
       "        (c_attn): Conv1D(nf=3072, nx=1024)\n",
       "        (c_proj): Conv1D(nf=1024, nx=1024)\n",
       "        (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "        (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp): GPT2MLP(\n",
       "        (c_fc): Conv1D(nf=4096, nx=1024)\n",
       "        (c_proj): Conv1D(nf=1024, nx=4096)\n",
       "        (act): NewGELUActivation()\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (ln_f): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       ")"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#from transformers import GPT2Model\n",
    "BASE_CONFIG = {\n",
    "    \"vocab_size\": 50257,     # Vocabulary size\n",
    "    \"context_length\": 1024,  # Context length\n",
    "    \"drop_rate\": 0.0,        # Dropout rate\n",
    "    \"qkv_bias\": True         # Query-key-value bias\n",
    "}\n",
    "\n",
    "# allowed model names\n",
    "model_names = {\n",
    "    \"gpt2-small (124M)\": \"openai-community/gpt2\",\n",
    "    \"gpt2-medium (355M)\": \"openai-community/gpt2-medium\",\n",
    "    \"gpt2-large (774M)\": \"openai-community/gpt2-large\",\n",
    "    \"gpt2-xl (1558M)\": \"openai-community/gpt2-xl\"\n",
    "}\n",
    "\n",
    "CHOOSE_MODEL = \"gpt2-medium (355M)\"\n",
    "\n",
    "model = GPT2Model.from_pretrained(model_names[CHOOSE_MODEL], cache_dir=\"checkpoints\")\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "ce63b04f-126b-43ac-bb3d-711e5d476b85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Convert the active sentence to passive: 'The chef cooks the meal every day.'\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "\n",
    "input_text = format_input(val_data[0])\n",
    "print(input_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "1fc6a1b7-8aea-4722-9939-83f728a95818",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "tuple indices must be integers or slices, not tuple",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[128], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m token_ids \u001b[38;5;241m=\u001b[39m \u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43midx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtext_to_token_ids\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_text\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m35\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcontext_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mBASE_CONFIG\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcontext_length\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43meos_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50256\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m generated_text \u001b[38;5;241m=\u001b[39m token_ids_to_text(token_ids, tokenizer)\n",
      "Cell \u001b[0;32mIn[90], line 8\u001b[0m, in \u001b[0;36mgenerate\u001b[0;34m(model, idx, max_new_tokens, context_size, temperature, top_k, eos_id)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m      7\u001b[0m     logits \u001b[38;5;241m=\u001b[39m model(idx_cond)\n\u001b[0;32m----> 8\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[43mlogits\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# New: Filter logits with top_k sampling\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m top_k \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;66;03m# Keep only top_k values\u001b[39;00m\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/llm-vBxNNxeW-py3.13/lib/python3.13/site-packages/transformers/utils/generic.py:435\u001b[0m, in \u001b[0;36mModelOutput.__getitem__\u001b[0;34m(self, k)\u001b[0m\n\u001b[1;32m    433\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m inner_dict[k]\n\u001b[1;32m    434\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 435\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_tuple\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[43mk\u001b[49m\u001b[43m]\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: tuple indices must be integers or slices, not tuple"
     ]
    }
   ],
   "source": [
    "token_ids = generate(\n",
    "    model=model,\n",
    "    idx=text_to_token_ids(input_text, tokenizer),\n",
    "    max_new_tokens=35,\n",
    "    context_size=BASE_CONFIG[\"context_length\"],\n",
    "    eos_id=50256,\n",
    ")\n",
    "generated_text = token_ids_to_text(token_ids, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "632a99f4-0e78-4703-82d4-6addbf5b8f13",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'generated_text' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[129], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m response_text \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m----> 2\u001b[0m     \u001b[43mgenerated_text\u001b[49m[\u001b[38;5;28mlen\u001b[39m(input_text):]\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m### Response:\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;241m.\u001b[39mstrip()\n\u001b[1;32m      5\u001b[0m )\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(response_text)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'generated_text' is not defined"
     ]
    }
   ],
   "source": [
    "response_text = (\n",
    "    generated_text[len(input_text):]\n",
    "    .replace(\"### Response:\", \"\")\n",
    "    .strip()\n",
    ")\n",
    "print(response_text)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
